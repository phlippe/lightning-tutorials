


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-PQBQ3CV');
  </script>
  <!-- End Google Tag Manager -->

  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="google-site-verification" content="okUst94cAlWSsUsGZTB4xSS4UKTtRV8Nu5XZ9pdd3Aw" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Initialization and Optimization &mdash; lightning-tutorials  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/icon.svg"/>
  
  
  
    <link rel="canonical" href="https://www.pytorchlightning.ai/notebooks/course_UvA-DL/initialization-and-optimization.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sphinx_paramlinks.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Introduction to PyTorch" href="introduction-to-pytorch.html" />
    <link rel="prev" title="Inception, ResNet and DenseNet" href="inception-resnet-densenet.html" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pt-lightning-sandbox.rtfd.io/en/latest/" aria-label="PyTorch">
      <!--  <img class="call-to-action-img" src="../../_static/images/logo-lightning-icon.png"/> -->
      </a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pt-lightning-sandbox.readthedocs.io/en/latest/introduction_guide.html">Get Started</a>
          </li>

          <li>
            <a href="https://www.pytorchlightning.ai/blog">Blog</a>
          </li>

          <li>
            <a href="https://pt-lightning-sandbox.readthedocs.io/en/latest/#tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch-lightning.readthedocs.io/en/stable/">
                  <span class="dropdown-title">PyTorch Lightning</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://torchmetrics.readthedocs.io/en/stable/">
                  <span class="dropdown-title">TorchMetrics</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-flash.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Flash</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-transformers.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Transformers</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-bolts.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Bolts</span>
                  <p></p>
                </a>
            </div>
          </li>

          <!--<li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pt-lightning-sandbox.readthedocs.io/en/latest/#community-examples">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>-->

          <li>
            <a href="https://github.com/PytorchLightning/lightning-sandbox">GitHub</a>
          </li>

          <li>
            <a href="https://www.grid.ai/">Grid.ai</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Start here</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../sample-template.html">How to write a PyTorch Lightning tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="activation-functions.html">Activation Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="autoregressive-image-modeling.html">Autoregressive Image Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="deep-autoencoders.html">Deep Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="deep-energy-based-generative-models.html">Deep Energy-Based Generative Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph-neural-networks.html">Basics of Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="inception-resnet-densenet.html">Inception, ResNet and DenseNet</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Initialization and Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction-to-pytorch.html">Introduction to PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="meta-learning.html">Meta-Learning - Learning to Learn</a></li>
<li class="toctree-l1"><a class="reference internal" href="normalizing-flows.html">Normalizing Flows for Image Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformers-and-MH-attention.html">Transformers and Multi-Head Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="vision-transformer.html">Vision Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lightning_examples/augmentation_kornia.html">GPU and batched data augmentation with Kornia and PyTorch-Lightning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lightning_examples/basic-gan.html">PyTorch Lightning Basic GAN Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lightning_examples/cifar10-baseline.html">PyTorch Lightning CIFAR10 ~94% Baseline Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lightning_examples/datamodules.html">PyTorch Lightning DataModules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lightning_examples/mnist-hello-world.html">Introduction to Pytorch Lightning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lightning_examples/mnist-tpu-training.html">TPU training with PyTorch Lightning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lightning_examples/reinforce-learning-DQN.html">How to train a Deep Q Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lightning_examples/text-transformers.html">Finetune Transformers Models with PyTorch Lightning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../starters/basic-gan.html">PyTorch Lightning Basic GAN Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../starters/cifar10-baseline.html">PyTorch Lightning CIFAR10 ~94% Baseline Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../starters/datamodules.html">PyTorch Lightning DataModules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../starters/mnist-hello-world.html">Introduction to Pytorch Lightning</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Initialization and Optimization</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../../_sources/notebooks/course_UvA-DL/initialization-and-optimization.ipynb.txt" rel="nofollow"><img src="../../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<section id="Initialization-and-Optimization">
<h1>Initialization and Optimization<a class="headerlink" href="#Initialization-and-Optimization" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><strong>Author:</strong> Phillip Lippe</p></li>
<li><p><strong>License:</strong> CC BY-SA</p></li>
<li><p><strong>Generated:</strong> 2021-09-02T20:41:01.573900</p></li>
</ul>
<p>In this tutorial, we will review techniques for optimization and initialization of neural networks. When increasing the depth of neural networks, there are various challenges we face. Most importantly, we need to have a stable gradient flow through the network, as otherwise, we might encounter vanishing or exploding gradients. This is why we will take a closer look at the following concepts: initialization and optimization. This notebook is part of a lecture series on Deep Learning at the
University of Amsterdam. The full list of tutorials can be found at <a class="reference external" href="https://uvadlc-notebooks.rtfd.io">https://uvadlc-notebooks.rtfd.io</a>.</p>
<hr class="docutils" />
<p>Open in <a class="reference external" href="https://colab.research.google.com/github/PytorchLightning/lightning-tutorials/blob/publication/.notebooks/course_UvA-DL/initialization-and-optimization.ipynb"><img alt="Open In Colab" src="data:image/png;base64,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" style="width: 117px; height: 20px;" /></a></p>
<p>Give us a ⭐ <a class="reference external" href="https://www.github.com/PytorchLightning/pytorch-lightning/">on Github</a> | Check out <a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/latest/">the documentation</a> | Join us <a class="reference external" href="https://join.slack.com/t/pytorch-lightning/shared_invite/zt-pw5v393p-qRaDgEk24~EjiZNBpSQFgQ">on Slack</a></p>
<section id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Permalink to this headline">¶</a></h2>
<p>This notebook requires some packages besides pytorch-lightning.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">!</span> pip install --quiet <span class="s2">&quot;seaborn&quot;</span> <span class="s2">&quot;pytorch-lightning&gt;=1.3&quot;</span> <span class="s2">&quot;torchvision&quot;</span> <span class="s2">&quot;torch&gt;=1.6, &lt;1.9&quot;</span> <span class="s2">&quot;torchmetrics&gt;=0.3&quot;</span> <span class="s2">&quot;matplotlib&quot;</span>
</pre></div>
</div>
</div>
<p>In the first half of the notebook, we will review different initialization techniques, and go step by step from the simplest initialization to methods that are nowadays used in very deep networks. In the second half, we focus on optimization comparing the optimizers SGD, SGD with Momentum, and Adam.</p>
<p>Let’s start with importing our standard libraries:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">urllib.request</span>
<span class="kn">from</span> <span class="nn">urllib.error</span> <span class="kn">import</span> <span class="n">HTTPError</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span> <span class="k">as</span> <span class="nn">data</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">set_matplotlib_formats</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">FashionMNIST</span>
<span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s2">&quot;svg&quot;</span><span class="p">,</span> <span class="s2">&quot;pdf&quot;</span><span class="p">)</span>  <span class="c1"># For export</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/tmp/ipykernel_1102/1682095326.py:24: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`
  set_matplotlib_formats(&#34;svg&#34;, &#34;pdf&#34;)  # For export
</pre></div></div>
</div>
<p>Instead of the <code class="docutils literal notranslate"><span class="pre">set_seed</span></code> function as in Tutorial 3, we can use PyTorch Lightning’s build-in function <code class="docutils literal notranslate"><span class="pre">pl.seed_everything</span></code>. We will reuse the path variables <code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code> and <code class="docutils literal notranslate"><span class="pre">CHECKPOINT_PATH</span></code> as in Tutorial 3. Adjust the paths if necessary.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Path to the folder where the datasets are/should be downloaded (e.g. MNIST)</span>
<span class="n">DATASET_PATH</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;PATH_DATASETS&quot;</span><span class="p">,</span> <span class="s2">&quot;data/&quot;</span><span class="p">)</span>
<span class="c1"># Path to the folder where the pretrained models are saved</span>
<span class="n">CHECKPOINT_PATH</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;PATH_CHECKPOINT&quot;</span><span class="p">,</span> <span class="s2">&quot;saved_models/InitOptim/&quot;</span><span class="p">)</span>

<span class="c1"># Seed everything</span>
<span class="n">pl</span><span class="o">.</span><span class="n">seed_everything</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Ensure that all operations are deterministic on GPU (if used) for reproducibility</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">determinstic</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Fetching the device that will be used throughout this notebook</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Using device&quot;</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Global seed set to 42
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Using device cuda:0
</pre></div></div>
</div>
<p>In the last part of the notebook, we will train models using three different optimizers. The pretrained models for those are downloaded below.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Github URL where saved models are stored for this tutorial</span>
<span class="n">base_url</span> <span class="o">=</span> <span class="s2">&quot;https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial4/&quot;</span>
<span class="c1"># Files to download</span>
<span class="n">pretrained_files</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;FashionMNIST_SGD.config&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FashionMNIST_SGD_results.json&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FashionMNIST_SGD.tar&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FashionMNIST_SGDMom.config&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FashionMNIST_SGDMom_results.json&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FashionMNIST_SGDMom.tar&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FashionMNIST_Adam.config&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FashionMNIST_Adam_results.json&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FashionMNIST_Adam.tar&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="c1"># Create checkpoint path if it doesn&#39;t exist yet</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">CHECKPOINT_PATH</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># For each file, check whether it already exists. If not, try downloading it.</span>
<span class="k">for</span> <span class="n">file_name</span> <span class="ow">in</span> <span class="n">pretrained_files</span><span class="p">:</span>
    <span class="n">file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">CHECKPOINT_PATH</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
        <span class="n">file_url</span> <span class="o">=</span> <span class="n">base_url</span> <span class="o">+</span> <span class="n">file_name</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Downloading </span><span class="si">{</span><span class="n">file_url</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">file_url</span><span class="p">,</span> <span class="n">file_path</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">HTTPError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="s2">&quot;Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">e</span><span class="p">,</span>
            <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial4/FashionMNIST_SGD.config...
Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial4/FashionMNIST_SGD_results.json...
Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial4/FashionMNIST_SGD.tar...
Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial4/FashionMNIST_SGDMom.config...
Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial4/FashionMNIST_SGDMom_results.json...
Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial4/FashionMNIST_SGDMom.tar...
Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial4/FashionMNIST_Adam.config...
Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial4/FashionMNIST_Adam_results.json...
Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial4/FashionMNIST_Adam.tar...
</pre></div></div>
</div>
</section>
<section id="Preparation">
<h2>Preparation<a class="headerlink" href="#Preparation" title="Permalink to this headline">¶</a></h2>
<p>Throughout this notebook, we will use a deep fully connected network, similar to our previous tutorial. We will also again apply the network to FashionMNIST, so you can relate to the results of Tutorial 3. We start by loading the FashionMNIST dataset:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>

<span></span><span class="c1"># Transformations applied on each image =&gt; first make them a tensor, then normalize them with mean 0 and std 1</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.2861</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3530</span><span class="p">,))])</span>

<span class="c1"># Loading the training dataset. We need to split it into a training and validation part</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">DATASET_PATH</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_set</span><span class="p">,</span> <span class="n">val_set</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">random_split</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="p">[</span><span class="mi">50000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">])</span>

<span class="c1"># Loading the test set</span>
<span class="n">test_set</span> <span class="o">=</span> <span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">DATASET_PATH</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We define a set of data loaders that we can use for various purposes later. Note that for actually training a model, we will use different data loaders with a lower batch size.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">val_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">val_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>In comparison to the previous tutorial, we have changed the parameters of the normalization transformation <code class="docutils literal notranslate"><span class="pre">transforms.Normalize</span></code>. The normalization is now designed to give us an expected mean of 0 and a standard deviation of 1 across pixels. This will be particularly relevant for the discussion about initialization we will look at below, and hence we change it here. It should be noted that in most classification tasks, both normalization techniques (between -1 and 1 or mean 0 and stddev 1)
have shown to work well. We can calculate the normalization parameters by determining the mean and standard deviation on the original images:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Std&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Mean 0.28604060411453247
Std 0.3530242443084717
</pre></div></div>
</div>
<p>We can verify the transformation by looking at the statistics of a single batch:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">imgs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean: </span><span class="si">{</span><span class="n">imgs</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">5.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard deviation: </span><span class="si">{</span><span class="n">imgs</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">5.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Maximum: </span><span class="si">{</span><span class="n">imgs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">5.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Minimum: </span><span class="si">{</span><span class="n">imgs</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">5.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Mean: 0.009
Standard deviation: 1.012
Maximum: 2.022
Minimum: -0.810
</pre></div></div>
</div>
<p>Note that the maximum and minimum are not 1 and -1 anymore, but shifted towards the positive values. This is because FashionMNIST contains a lot of black pixels, similar to MNIST.</p>
<p>Next, we create a linear neural network. We use the same setup as in the previous tutorial.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">BaseNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">act_fn</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hidden_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            act_fn: Object of the activation function that should be used as non-linearity in the network.</span>
<span class="sd">            input_size: Size of the input images in pixels</span>
<span class="sd">            num_classes: Number of classes we want to predict</span>
<span class="sd">            hidden_sizes: A list of integers specifying the hidden layer sizes in the NN</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Create the network based on the specified hidden sizes</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">hidden_sizes</span>
        <span class="k">for</span> <span class="n">layer_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)):</span>
            <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">layer_index</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">layer_sizes</span><span class="p">[</span><span class="n">layer_index</span><span class="p">]),</span> <span class="n">act_fn</span><span class="p">]</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">num_classes</span><span class="p">)]</span>
        <span class="c1"># A module list registers a list of modules as submodules (e.g. for parameters)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;act_fn&quot;</span><span class="p">:</span> <span class="n">act_fn</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
            <span class="s2">&quot;input_size&quot;</span><span class="p">:</span> <span class="n">input_size</span><span class="p">,</span>
            <span class="s2">&quot;num_classes&quot;</span><span class="p">:</span> <span class="n">num_classes</span><span class="p">,</span>
            <span class="s2">&quot;hidden_sizes&quot;</span><span class="p">:</span> <span class="n">hidden_sizes</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
<p>For the activation functions, we make use of PyTorch’s <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> library instead of implementing ourselves. However, we also define an <code class="docutils literal notranslate"><span class="pre">Identity</span></code> activation function. Although this activation function would significantly limit the network’s modeling capabilities, we will use it in the first steps of our discussion about initialization (for simplicity).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">Identity</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="n">act_fn_by_name</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;tanh&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span> <span class="s2">&quot;identity&quot;</span><span class="p">:</span> <span class="n">Identity</span><span class="p">}</span>
</pre></div>
</div>
</div>
<p>Finally, we define a few plotting functions that we will use for our discussions. These functions help us to (1) visualize the weight/parameter distribution inside a network, (2) visualize the gradients that the parameters at different layers receive, and (3) the activations, i.e. the output of the linear layers. The detailed code is not important, but feel free to take a closer look if interested.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1">##############################################################</span>


<span class="k">def</span> <span class="nf">plot_dists</span><span class="p">(</span><span class="n">val_dict</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;count&quot;</span><span class="p">,</span> <span class="n">use_kde</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_dict</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">columns</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">columns</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>
    <span class="n">fig_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">val_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
        <span class="n">key_ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">fig_index</span> <span class="o">%</span> <span class="n">columns</span><span class="p">]</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span>
            <span class="n">val_dict</span><span class="p">[</span><span class="n">key</span><span class="p">],</span>
            <span class="n">ax</span><span class="o">=</span><span class="n">key_ax</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span>
            <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
            <span class="n">stat</span><span class="o">=</span><span class="n">stat</span><span class="p">,</span>
            <span class="n">kde</span><span class="o">=</span><span class="n">use_kde</span> <span class="ow">and</span> <span class="p">((</span><span class="n">val_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">val_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mf">1e-8</span><span class="p">),</span>
        <span class="p">)</span>  <span class="c1"># Only plot kde if there is variance</span>
        <span class="n">hidden_dim_str</span> <span class="o">=</span> <span class="p">(</span>
            <span class="sa">r</span><span class="s2">&quot;(</span><span class="si">%i</span><span class="s2"> $\to$ </span><span class="si">%i</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">val_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">val_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
        <span class="p">)</span>
        <span class="n">key_ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">hidden_dim_str</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">xlabel</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key_ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">)</span>
        <span class="n">fig_index</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fig</span>


<span class="c1">##############################################################</span>


<span class="k">def</span> <span class="nf">visualize_weight_distribution</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">):</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.bias&quot;</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="n">key_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Layer </span><span class="si">{</span><span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">weights</span><span class="p">[</span><span class="n">key_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># Plotting</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plot_dists</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Weight vals&quot;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Weight distribution&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>


<span class="c1">##############################################################</span>


<span class="k">def</span> <span class="nf">visualize_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        net: Object of class BaseNetwork</span>
<span class="sd">        color: Color in which we want to visualize the histogram (for easier separation of activation functions)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">small_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">small_loader</span><span class="p">))</span>
    <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Pass one batch through the network, and calculate the gradients for the weights</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>  <span class="c1"># Same as nn.CrossEntropyLoss, but as a function instead of module</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">name</span><span class="p">:</span> <span class="n">params</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;weight&quot;</span> <span class="ow">in</span> <span class="n">name</span>
    <span class="p">}</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Plotting</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plot_dists</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Grad magnitude&quot;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Gradient distribution&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">print_variance</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">grads</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> - Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="c1">##############################################################</span>


<span class="k">def</span> <span class="nf">visualize_activations</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">small_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">small_loader</span><span class="p">))</span>
    <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Pass one batch through the network, and calculate the gradients for the weights</span>
    <span class="n">feats</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">imgs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">layer_index</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="n">feats</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">feats</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="n">activations</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Layer </span><span class="si">{</span><span class="n">layer_index</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">feats</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># Plotting</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plot_dists</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Activation vals&quot;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Activation distribution&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">print_variance</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">activations</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> - Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="n">key</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="c1">##############################################################</span>
</pre></div>
</div>
</div>
</section>
<section id="Initialization">
<h2>Initialization<a class="headerlink" href="#Initialization" title="Permalink to this headline">¶</a></h2>
<p>Before starting our discussion about initialization, it should be noted that there exist many very good blog posts about the topic of neural network initialization (for example <a class="reference external" href="https://www.deeplearning.ai/ai-notes/initialization/">deeplearning.ai</a>, or a more <a class="reference external" href="https://pouannes.github.io/blog/initialization/#mjx-eqn-eqfwd_K">math-focused blog post</a>). In case something remains unclear after this tutorial, we recommend skimming through these blog posts as well.</p>
<p>When initializing a neural network, there are a few properties we would like to have. First, the variance of the input should be propagated through the model to the last layer, so that we have a similar standard deviation for the output neurons. If the variance would vanish the deeper we go in our model, it becomes much harder to optimize the model as the input to the next layer is basically a single constant value. Similarly, if the variance increases, it is likely to explode (i.e. head to
infinity) the deeper we design our model. The second property we look out for in initialization techniques is a gradient distribution with equal variance across layers. If the first layer receives much smaller gradients than the last layer, we will have difficulties in choosing an appropriate learning rate.</p>
<p>As a starting point for finding a good method, we will analyze different initialization based on our linear neural network with no activation function (i.e. an identity). We do this because initializations depend on the specific activation function used in the network, and we can adjust the initialization schemes later on for our specific choice.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">BaseNetwork</span><span class="p">(</span><span class="n">act_fn</span><span class="o">=</span><span class="n">Identity</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<section id="Constant-initialization">
<h3>Constant initialization<a class="headerlink" href="#Constant-initialization" title="Permalink to this headline">¶</a></h3>
<p>The first initialization we can consider is to initialize all weights with the same constant value. Intuitively, setting all weights to zero is not a good idea as the propagated gradient will be zero. However, what happens if we set all weights to a value slightly larger or smaller than 0? To find out, we can implement a function for setting all parameters below and visualize the gradients.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">const_init</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">fill</span><span class="p">)</span>


<span class="n">const_init</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="mf">0.005</span><span class="p">)</span>
<span class="n">visualize_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">visualize_activations</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_27_0.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_27_0.svg" /></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_27_1.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_27_1.svg" /></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Layer 0 - Variance: 2.0582756996154785
Layer 2 - Variance: 13.489118576049805
Layer 4 - Variance: 22.100566864013672
Layer 6 - Variance: 36.209571838378906
Layer 8 - Variance: 14.831439018249512
</pre></div></div>
</div>
<p>As we can see, only the first and the last layer have diverse gradient distributions while the other three layers have the same gradient for all weights (note that this value is unequal 0, but often very close to it). Having the same gradient for parameters that have been initialized with the same values means that we will always have the same value for those parameters. This would make our layer useless and reduce our effective number of parameters to 1. Thus, we cannot use a constant
initialization to train our networks.</p>
</section>
<section id="Constant-variance">
<h3>Constant variance<a class="headerlink" href="#Constant-variance" title="Permalink to this headline">¶</a></h3>
<p>From the experiment above, we have seen that a constant value is not working. So instead, how about we initialize the parameters by randomly sampling from a distribution like a Gaussian? The most intuitive way would be to choose one variance that is used for all layers in the network. Let’s implement it below, and visualize the activation distribution across layers.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">var_init</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">)</span>


<span class="n">var_init</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">visualize_activations</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_30_0.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_30_0.svg" /></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Layer 0 - Variance: 0.07831248641014099
Layer 2 - Variance: 0.004064005799591541
Layer 4 - Variance: 0.00022317888215184212
Layer 6 - Variance: 0.00011556116805877537
Layer 8 - Variance: 8.162161248037592e-05
</pre></div></div>
</div>
<p>The variance of the activation becomes smaller and smaller across layers, and almost vanishes in the last layer. Alternatively, we could use a higher standard deviation:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">var_init</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">visualize_activations</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_32_0.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_32_0.svg" /></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Layer 0 - Variance: 8.082208633422852
Layer 2 - Variance: 37.87363815307617
Layer 4 - Variance: 96.36101531982422
Layer 6 - Variance: 237.2630615234375
Layer 8 - Variance: 303.44244384765625
</pre></div></div>
</div>
<p>With a higher standard deviation, the activations are likely to explode. You can play around with the specific standard deviation values, but it will be hard to find one that gives us a good activation distribution across layers and is very specific to our model. If we would change the hidden sizes or number of layers, you would have to search all over again, which is neither efficient nor recommended.</p>
</section>
<section id="How-to-find-appropriate-initialization-values">
<h3>How to find appropriate initialization values<a class="headerlink" href="#How-to-find-appropriate-initialization-values" title="Permalink to this headline">¶</a></h3>
<p>From our experiments above, we have seen that we need to sample the weights from a distribution, but are not sure which one exactly. As a next step, we will try to find the optimal initialization from the perspective of the activation distribution. For this, we state two requirements:</p>
<ol class="arabic simple">
<li><p>The mean of the activations should be zero</p></li>
<li><p>The variance of the activations should stay the same across every layer</p></li>
</ol>
<p>Suppose we want to design an initialization for the following layer: <img class="math" src="../../_images/math/e1104f21549a24e4ea083961eaa9accdd97fecee.png" alt="y=Wx+b"/> with <img class="math" src="../../_images/math/6db681051b684ee5379fa7befddac2e1d9d2cf09.png" alt="y\in\mathbb{R}^{d_y}"/>, <img class="math" src="../../_images/math/a0acc260617739a081f927acdc805f29bfbf265f.png" alt="x\in\mathbb{R}^{d_x}"/>. Our goal is that the variance of each element of <img class="math" src="../../_images/math/1b5e577d6216dca3af7d87aa122a0b9b360d6cb3.png" alt="y"/> is the same as the input, i.e. <img class="math" src="../../_images/math/33cdf87e872ed4bbf234af9eb9b228582a7db574.png" alt="\text{Var}(y_i)=\text{Var}(x_i)=\sigma_x^{2}"/>, and that the mean is zero. We assume <img class="math" src="../../_images/math/888f7c323ac0341871e867220ae2d76467d74d6e.png" alt="x"/> to also have a mean of zero, because, in deep neural networks, <img class="math" src="../../_images/math/1b5e577d6216dca3af7d87aa122a0b9b360d6cb3.png" alt="y"/> would be the input of another layer. This requires the bias and weight to have an
expectation of 0. Actually, as <img class="math" src="../../_images/math/68c7c8c65602677ab56cf7fd88002023f0edc575.png" alt="b"/> is a single element per output neuron and is constant across different inputs, we set it to 0 overall.</p>
<p>Next, we need to calculate the variance with which we need to initialize the weight parameters. Along the calculation, we will need to following variance rule: given two independent variables, the variance of their product is <img class="math" src="../../_images/math/697d3036f08b3edbd87636fbc33a5e9d88ff126d.png" alt="\text{Var}(X\cdot Y) = \mathbb{E}(Y)^2\text{Var}(X) + \mathbb{E}(X)^2\text{Var}(Y) + \text{Var}(X)\text{Var}(Y) = \mathbb{E}(Y^2)\mathbb{E}(X^2)-\mathbb{E}(Y)^2\mathbb{E}(X)^2"/> (<img class="math" src="../../_images/math/ed38fa24f1c94891bd312012aab3f6673be3eb83.png" alt="X"/> and <img class="math" src="../../_images/math/7daf0d4815e763eb90f0d5f1dc406f668c1e21db.png" alt="Y"/> are not refering to <img class="math" src="../../_images/math/888f7c323ac0341871e867220ae2d76467d74d6e.png" alt="x"/> and <img class="math" src="../../_images/math/1b5e577d6216dca3af7d87aa122a0b9b360d6cb3.png" alt="y"/>, but any random
variable).</p>
<p>The needed variance of the weights, <img class="math" src="../../_images/math/0c39d193b77bd7c3b15f56044ee42e27f59dd2ed.png" alt="\text{Var}(w_{ij})"/>, is calculated as follows:</p>
<div class="math">
<p><img src="../../_images/math/4cab45d61e1bccc6997c86354bc873b7a18396a1.png" alt="\begin{split}
    y_i &amp; = \sum_{j} w_{ij}x_{j}\hspace{10mm}\text{Calculation of a single output neuron without bias}\\
    \text{Var}(y_i) = \sigma_x^{2} &amp; = \text{Var}\left(\sum_{j} w_{ij}x_{j}\right)\\
    &amp; = \sum_{j} \text{Var}(w_{ij}x_{j}) \hspace{10mm}\text{Inputs and weights are independent of each other}\\
    &amp; = \sum_{j} \text{Var}(w_{ij})\cdot\text{Var}(x_{j}) \hspace{10mm}\text{Variance rule (see above) with expectations being zero}\\
    &amp; = d_x \cdot \text{Var}(w_{ij})\cdot\text{Var}(x_{j}) \hspace{10mm}\text{Variance equal for all $d_x$ elements}\\
    &amp; = \sigma_x^{2} \cdot d_x \cdot \text{Var}(w_{ij})\\
    \Rightarrow \text{Var}(w_{ij}) = \sigma_{W}^2 &amp; = \frac{1}{d_x}\\
\end{split}"/></p>
</div><p>Thus, we should initialize the weight distribution with a variance of the inverse of the input dimension <img class="math" src="../../_images/math/8008319ad0df859eba4cb116ef0aaf6654ccdbd7.png" alt="d_x"/>. Let’s implement it below and check whether this holds:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">equal_var_init</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.bias&quot;</span><span class="p">):</span>
            <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">std</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>


<span class="n">equal_var_init</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">visualize_weight_distribution</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">visualize_activations</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_35_0.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_35_0.svg" /></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_35_1.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_35_1.svg" /></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Layer 0 - Variance: 1.0088235139846802
Layer 2 - Variance: 1.0696827173233032
Layer 4 - Variance: 1.125657081604004
Layer 6 - Variance: 1.1308791637420654
Layer 8 - Variance: 1.0503977537155151
</pre></div></div>
</div>
<p>As we expected, the variance stays indeed constant across layers. Note that our initialization does not restrict us to a normal distribution, but allows any other distribution with a mean of 0 and variance of <img class="math" src="../../_images/math/9316a0047613c51a8991cd4f1ef1890bb3aa91fd.png" alt="1/d_x"/>. You often see that a uniform distribution is used for initialization. A small benefit of using a uniform instead of a normal distribution is that we can exclude the chance of initializing very large or small weights.</p>
<p>Besides the variance of the activations, another variance we would like to stabilize is the one of the gradients. This ensures a stable optimization for deep networks. It turns out that we can do the same calculation as above starting from <img class="math" src="../../_images/math/0e4c8ddcce0719b9eda570bdef39cf15296781e8.png" alt="\Delta x=W\Delta y"/>, and come to the conclusion that we should initialize our layers with <img class="math" src="../../_images/math/4e8433574b85859811905991c4a547758943ccb8.png" alt="1/d_y"/> where <img class="math" src="../../_images/math/ea00b0303389f6be9089989b68a8f33b068ee15b.png" alt="d_y"/> is the number of output neurons. You can do the calculation as a practice, or check a thorough explanation in <a class="reference external" href="https://pouannes.github.io/blog/initialization/#mjx-eqn-eqfwd_K">this blog
post</a>. As a compromise between both constraints, <a class="reference external" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi">Glorot and Bengio (2010)</a> proposed to use the harmonic mean of both values. This leads us to the well-known Xavier initialization:</p>
<div class="math">
<p><img src="../../_images/math/7962336a6de38100e7366606da9a0cb3300a3432.png" alt="W\sim \mathcal{N}\left(0,\frac{2}{d_x+d_y}\right)"/></p>
</div><p>If we use a uniform distribution, we would initialize the weights with:</p>
<div class="math">
<p><img src="../../_images/math/fd7d732fe3ecb390f64710371eaf12120da9eb69.png" alt="W\sim U\left[-\frac{\sqrt{6}}{\sqrt{d_x+d_y}}, \frac{\sqrt{6}}{\sqrt{d_x+d_y}}\right]"/></p>
</div><p>Let’s shortly implement it and validate its effectiveness:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">xavier_init</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.bias&quot;</span><span class="p">):</span>
            <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bound</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>


<span class="n">xavier_init</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">visualize_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">visualize_activations</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_37_0.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_37_0.svg" /></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
layers.0.weight - Variance: 0.0003991015546489507
layers.2.weight - Variance: 0.0007022571517154574
layers.4.weight - Variance: 0.0009397325338795781
layers.6.weight - Variance: 0.0014803955564275384
layers.8.weight - Variance: 0.012549502775073051
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_37_2.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_37_2.svg" /></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Layer 0 - Variance: 1.2209526300430298
Layer 2 - Variance: 1.5839706659317017
Layer 4 - Variance: 1.5429933071136475
Layer 6 - Variance: 2.021383047103882
Layer 8 - Variance: 2.6867828369140625
</pre></div></div>
</div>
<p>We see that the Xavier initialization balances the variance of gradients and activations. Note that the significantly higher variance for the output layer is due to the large difference of input and output dimension (<img class="math" src="../../_images/math/c4293dcefeb893a0916cec458131373fdeb8f758.png" alt="128"/> vs <img class="math" src="../../_images/math/475d5964a90500c461804bca3d965ff8679e248c.png" alt="10"/>). However, we currently assumed the activation function to be linear. So what happens if we add a non-linearity? In a tanh-based network, a common assumption is that for small values during the initial steps in training, the <img class="math" src="../../_images/math/04b8b6fadb091b97688d504cc35f02d8856b1bbe.png" alt="\tanh"/> works as a linear
function such that we don’t have to adjust our calculation. We can check if that is the case for us as well:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">BaseNetwork</span><span class="p">(</span><span class="n">act_fn</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">xavier_init</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">visualize_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">visualize_activations</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_39_0.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_39_0.svg" /></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
layers.0.weight - Variance: 2.1826384909218177e-05
layers.2.weight - Variance: 3.5952674807049334e-05
layers.4.weight - Variance: 4.872870340477675e-05
layers.6.weight - Variance: 6.269156438065693e-05
layers.8.weight - Variance: 0.0004620618128683418
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_39_2.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_39_2.svg" /></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Layer 0 - Variance: 1.2046984434127808
Layer 2 - Variance: 0.5917537212371826
Layer 4 - Variance: 0.2959783673286438
Layer 6 - Variance: 0.24997730553150177
Layer 8 - Variance: 0.2727622389793396
</pre></div></div>
</div>
<p>Although the variance decreases over depth, it is apparent that the activation distribution becomes more focused on the low values. Therefore, our variance will stabilize around 0.25 if we would go even deeper. Hence, we can conclude that the Xavier initialization works well for Tanh networks. But what about ReLU networks? Here, we cannot take the previous assumption of the non-linearity becoming linear for small values. The ReLU activation function sets (in expectation) half of the inputs to 0
so that also the expectation of the input is not zero. However, as long as the expectation of <img class="math" src="../../_images/math/1fbee781f84569077719a167b64e12064360fac1.png" alt="W"/> is zero and <img class="math" src="../../_images/math/909e24d245f258d326cb27b9517126ea124435db.png" alt="b=0"/>, the expectation of the output is zero. The part where the calculation of the ReLU initialization differs from the identity is when determining <img class="math" src="../../_images/math/ddc90fca2320bd6c61229410d323a8352c4b90d5.png" alt="\text{Var}(w_{ij}x_{j})"/>:</p>
<div class="math">
<p><img src="../../_images/math/cad8ceb41f952114ec8304403e7ebc9e4caee17c.png" alt="\text{Var}(w_{ij}x_{j})=\underbrace{\mathbb{E}[w_{ij}^2]}_{=\text{Var}(w_{ij})}\mathbb{E}[x_{j}^2]-\underbrace{\mathbb{E}[w_{ij}]^2}_{=0}\mathbb{E}[x_{j}]^2=\text{Var}(w_{ij})\mathbb{E}[x_{j}^2]"/></p>
</div><p>If we assume now that <img class="math" src="../../_images/math/888f7c323ac0341871e867220ae2d76467d74d6e.png" alt="x"/> is the output of a ReLU activation (from a previous layer, <img class="math" src="../../_images/math/623fa87316ef78511e23597b9207491ba046c39d.png" alt="x=max(0,\tilde{y})"/>), we can calculate the expectation as follows:</p>
<div class="math">
<p><img src="../../_images/math/69d6adf0f566822c85a3a1d0859bbea78c519137.png" alt="\begin{split}
    \mathbb{E}[x^2] &amp; =\mathbb{E}[\max(0,\tilde{y})^2]\\
                    &amp; =\frac{1}{2}\mathbb{E}[{\tilde{y}}^2]\hspace{2cm}\tilde{y}\text{ is zero-centered and symmetric}\\
                    &amp; =\frac{1}{2}\text{Var}(\tilde{y})
\end{split}"/></p>
</div><p>Thus, we see that we have an additional factor of 1/2 in the equation, so that our desired weight variance becomes <img class="math" src="../../_images/math/d4fe524ed43f946d528d4286cb33d31fa2b78eb7.png" alt="2/d_x"/>. This gives us the Kaiming initialization (see <a class="reference external" href="https://arxiv.org/pdf/1502.01852.pdf">He, K. et al. (2015)</a>). Note that the Kaiming initialization does not use the harmonic mean between input and output size. In their paper (Section 2.2, Backward Propagation, last paragraph), they argue that using <img class="math" src="../../_images/math/8008319ad0df859eba4cb116ef0aaf6654ccdbd7.png" alt="d_x"/> or <img class="math" src="../../_images/math/ea00b0303389f6be9089989b68a8f33b068ee15b.png" alt="d_y"/> both lead to stable gradients throughout
the network, and only depend on the overall input and output size of the network. Hence, we can use here only the input <img class="math" src="../../_images/math/8008319ad0df859eba4cb116ef0aaf6654ccdbd7.png" alt="d_x"/>:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">kaiming_init</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.bias&quot;</span><span class="p">):</span>
            <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;layers.0&quot;</span><span class="p">):</span>  <span class="c1"># The first layer does not have ReLU applied on its input</span>
            <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">BaseNetwork</span><span class="p">(</span><span class="n">act_fn</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">kaiming_init</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">visualize_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">visualize_activations</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_41_0.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_41_0.svg" /></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
layers.0.weight - Variance: 3.414905950194225e-05
layers.2.weight - Variance: 3.843478407361545e-05
layers.4.weight - Variance: 4.713246744358912e-05
layers.6.weight - Variance: 0.00010930334246950224
layers.8.weight - Variance: 0.0017839515348896384
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_41_2.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_41_2.svg" /></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Layer 0 - Variance: 1.0256913900375366
Layer 2 - Variance: 1.0101124048233032
Layer 4 - Variance: 1.0158814191818237
Layer 6 - Variance: 1.1398581266403198
Layer 8 - Variance: 0.46903371810913086
</pre></div></div>
</div>
<p>The variance stays stable across layers. We can conclude that the Kaiming initialization indeed works well for ReLU-based networks. Note that for Leaky-ReLU etc., we have to slightly adjust the factor of <img class="math" src="../../_images/math/d94997a2318fec7e8e5bc4d8d79bb633675f9411.png" alt="2"/> in the variance as half of the values are not set to zero anymore. PyTorch provides a function to calculate this factor for many activation function, see <code class="docutils literal notranslate"><span class="pre">torch.nn.init.calculate_gain</span></code> (<a class="reference external" href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.calculate_gain">link</a>).</p>
</section>
</section>
<section id="Optimization">
<h2>Optimization<a class="headerlink" href="#Optimization" title="Permalink to this headline">¶</a></h2>
<p>Besides initialization, selecting a suitable optimization algorithm can be an important choice for deep neural networks. Before taking a closer look at them, we should define code for training the models. Most of the following code is copied from the previous tutorial, and only slightly altered to fit our needs.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">_get_config_file</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">model_name</span> <span class="o">+</span> <span class="s2">&quot;.config&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_model_file</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">model_name</span> <span class="o">+</span> <span class="s2">&quot;.tar&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_result_file</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">model_name</span> <span class="o">+</span> <span class="s2">&quot;_results.json&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">net</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">config_file</span> <span class="o">=</span> <span class="n">_get_config_file</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">model_name</span><span class="p">)</span>
    <span class="n">model_file</span> <span class="o">=</span> <span class="n">_get_model_file</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">model_name</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span>
        <span class="n">config_file</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s1">&#39;Could not find the config file &quot;</span><span class="si">{</span><span class="n">config_file</span><span class="si">}</span><span class="s1">&quot;. Are you sure this is the correct path and you have your model config stored here?&#39;</span>
    <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span>
        <span class="n">model_file</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s1">&#39;Could not find the model file &quot;</span><span class="si">{</span><span class="n">model_file</span><span class="si">}</span><span class="s1">&quot;. Are you sure this is the correct path and you have your model stored here?&#39;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">config_file</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">config_dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">net</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">act_fn_name</span> <span class="o">=</span> <span class="n">config_dict</span><span class="p">[</span><span class="s2">&quot;act_fn&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">act_fn_name</span> <span class="ow">in</span> <span class="n">act_fn_by_name</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s1">&#39;Unknown activation function &quot;</span><span class="si">{</span><span class="n">act_fn_name</span><span class="si">}</span><span class="s1">&quot;. Please add it to the &quot;act_fn_by_name&quot; dict.&#39;</span>
        <span class="n">act_fn</span> <span class="o">=</span> <span class="n">act_fn_by_name</span><span class="p">[</span><span class="n">act_fn_name</span><span class="p">]()</span>
        <span class="n">net</span> <span class="o">=</span> <span class="n">BaseNetwork</span><span class="p">(</span><span class="n">act_fn</span><span class="o">=</span><span class="n">act_fn</span><span class="p">,</span> <span class="o">**</span><span class="n">config_dict</span><span class="p">)</span>
    <span class="n">net</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_file</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">net</span>


<span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">model_path</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
    <span class="n">config_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">config_file</span> <span class="o">=</span> <span class="n">_get_config_file</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">model_name</span><span class="p">)</span>
    <span class="n">model_file</span> <span class="o">=</span> <span class="n">_get_model_file</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">model_name</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">config_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">config_dict</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">model_file</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">optim_func</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Train a model on the training set of FashionMNIST.</span>

<span class="sd">    Args:</span>
<span class="sd">        net: Object of BaseNetwork</span>
<span class="sd">        model_name: (str) Name of the model, used for creating the checkpoint names</span>
<span class="sd">        max_epochs: Number of epochs we want to (maximally) train for</span>
<span class="sd">        patience: If the performance on the validation set has not improved for #patience epochs, we stop training early</span>
<span class="sd">        batch_size: Size of batches used in training</span>
<span class="sd">        overwrite: Determines how to handle the case when there already exists a checkpoint. If True, it will be overwritten. Otherwise, we skip training.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">file_exists</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">_get_model_file</span><span class="p">(</span><span class="n">CHECKPOINT_PATH</span><span class="p">,</span> <span class="n">model_name</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">file_exists</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">overwrite</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Model file of &quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s1">&quot; already exists. Skipping training...&#39;</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">_get_result_file</span><span class="p">(</span><span class="n">CHECKPOINT_PATH</span><span class="p">,</span> <span class="n">model_name</span><span class="p">))</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">results</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">file_exists</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model file exists, but will be overwritten...&quot;</span><span class="p">)</span>

        <span class="c1"># Defining optimizer, loss and data loader</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim_func</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="n">loss_module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
        <span class="n">train_loader_local</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="n">results</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">val_scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">train_losses</span><span class="p">,</span> <span class="n">train_scores</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="n">best_val_epoch</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_epochs</span><span class="p">):</span>
            <span class="n">train_acc</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="n">epoch_losses</span> <span class="o">=</span> <span class="n">epoch_iteration</span><span class="p">(</span>
                <span class="n">net</span><span class="p">,</span> <span class="n">loss_module</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader_local</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">epoch</span>
            <span class="p">)</span>
            <span class="n">train_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span>
            <span class="n">val_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_acc</span><span class="p">)</span>
            <span class="n">train_losses</span> <span class="o">+=</span> <span class="n">epoch_losses</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_scores</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">val_acc</span> <span class="o">&gt;</span> <span class="n">val_scores</span><span class="p">[</span><span class="n">best_val_epoch</span><span class="p">]:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">   (New best performance, saving model...)&quot;</span><span class="p">)</span>
                <span class="n">save_model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">CHECKPOINT_PATH</span><span class="p">,</span> <span class="n">model_name</span><span class="p">)</span>
                <span class="n">best_val_epoch</span> <span class="o">=</span> <span class="n">epoch</span>

    <span class="k">if</span> <span class="n">results</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">load_model</span><span class="p">(</span><span class="n">CHECKPOINT_PATH</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">net</span><span class="o">=</span><span class="n">net</span><span class="p">)</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">test_model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;test_acc&quot;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">,</span>
            <span class="s2">&quot;val_scores&quot;</span><span class="p">:</span> <span class="n">val_scores</span><span class="p">,</span>
            <span class="s2">&quot;train_losses&quot;</span><span class="p">:</span> <span class="n">train_losses</span><span class="p">,</span>
            <span class="s2">&quot;train_scores&quot;</span><span class="p">:</span> <span class="n">train_scores</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">_get_result_file</span><span class="p">(</span><span class="n">CHECKPOINT_PATH</span><span class="p">,</span> <span class="n">model_name</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

    <span class="c1"># Plot a curve of the validation accuracy</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s2">&quot;train_scores&quot;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;train_scores&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s2">&quot;val_scores&quot;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;val_scores&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Val&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epochs&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Validation accuracy&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s2">&quot;val_scores&quot;</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s2">&quot;train_scores&quot;</span><span class="p">])</span> <span class="o">*</span> <span class="mf">1.01</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Validation performance of </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">((</span><span class="sa">f</span><span class="s2">&quot; Test accuracy: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;test_acc&#39;</span><span class="p">]</span><span class="o">*</span><span class="mf">100.0</span><span class="si">:</span><span class="s2">4.2f</span><span class="si">}</span><span class="s2">% &quot;</span><span class="p">)</span><span class="o">.</span><span class="n">center</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="s2">&quot;=&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>


<span class="k">def</span> <span class="nf">epoch_iteration</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_module</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader_local</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
    <span class="c1">############</span>
    <span class="c1"># Training #</span>
    <span class="c1">############</span>
    <span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">true_preds</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">epoch_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_loader_local</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">t</span><span class="p">:</span>
        <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_module</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="c1"># Record statistics during training</span>
        <span class="n">true_preds</span> <span class="o">+=</span> <span class="p">(</span><span class="n">preds</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">t</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: loss=</span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">4.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">epoch_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">train_acc</span> <span class="o">=</span> <span class="n">true_preds</span> <span class="o">/</span> <span class="n">count</span>

    <span class="c1">##############</span>
    <span class="c1"># Validation #</span>
    <span class="c1">##############</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">test_model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;[Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="s2">2i</span><span class="si">}</span><span class="s2">] Training accuracy: </span><span class="si">{</span><span class="n">train_acc</span><span class="o">*</span><span class="mf">100.0</span><span class="si">:</span><span class="s2">05.2f</span><span class="si">}</span><span class="s2">%, Validation accuracy: </span><span class="si">{</span><span class="n">val_acc</span><span class="o">*</span><span class="mf">100.0</span><span class="si">:</span><span class="s2">05.2f</span><span class="si">}</span><span class="s2">%&quot;</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="n">epoch_losses</span>


<span class="k">def</span> <span class="nf">test_model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Test a model on a specified dataset.</span>

<span class="sd">    Args:</span>
<span class="sd">        net: Trained model of type BaseNetwork</span>
<span class="sd">        data_loader: DataLoader object of the dataset to test on (validation or test)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">true_preds</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
        <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">true_preds</span> <span class="o">+=</span> <span class="p">(</span><span class="n">preds</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">count</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">true_preds</span> <span class="o">/</span> <span class="n">count</span>
    <span class="k">return</span> <span class="n">test_acc</span>
</pre></div>
</div>
</div>
<p>First, we need to understand what an optimizer actually does. The optimizer is responsible to update the network’s parameters given the gradients. Hence, we effectively implement a function <img class="math" src="../../_images/math/a9a0b8ad71703b815509633298726502ea08f69d.png" alt="w^{t} = f(w^{t-1}, g^{t}, ...)"/> with <img class="math" src="../../_images/math/cc87ee665749db882f94e0d3707eb23e39638650.png" alt="w"/> being the parameters, and <img class="math" src="../../_images/math/aae18d2de9aa655108aa435aa095caa271582315.png" alt="g^{t} = \nabla_{w^{(t-1)}} \mathcal{L}^{(t)}"/> the gradients at time step <img class="math" src="../../_images/math/907a4add6d5db5b7f197f7924f1371b8ac404fe6.png" alt="t"/>. A common, additional parameter to this function is the learning rate, here denoted by <img class="math" src="../../_images/math/97db043c7cba573ac389b71add78b048077e8a13.png" alt="\eta"/>. Usually, the learning rate can be seen
as the “step size” of the update. A higher learning rate means that we change the weights more in the direction of the gradients, a smaller means we take shorter steps.</p>
<p>As most optimizers only differ in the implementation of <img class="math" src="../../_images/math/5b7752c757e0b691a80ab8227eadb8a8389dc58a.png" alt="f"/>, we can define a template for an optimizer in PyTorch below. We take as input the parameters of a model and a learning rate. The function <code class="docutils literal notranslate"><span class="pre">zero_grad</span></code> sets the gradients of all parameters to zero, which we have to do before calling <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>. Finally, the <code class="docutils literal notranslate"><span class="pre">step()</span></code> function tells the optimizer to update all weights based on their gradients. The template is setup below:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">OptimizerTemplate</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>

    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Set gradients of all parameters to zero</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span>  <span class="c1"># For second-order optimizers important</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Apply update step to all parameters</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># We skip parameters without any gradients</span>
                <span class="k">continue</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_param</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="c1"># To be implemented in optimizer-specific classes</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</pre></div>
</div>
</div>
<p>The first optimizer we are going to implement is the standard Stochastic Gradient Descent (SGD). SGD updates the parameters using the following equation:</p>
<div class="math">
<p><img src="../../_images/math/ba1f381be4af89b9fa7dc2e4e1528ab6dd3ca95c.png" alt="\begin{split}
    w^{(t)} &amp; = w^{(t-1)} - \eta \cdot g^{(t)}
\end{split}"/></p>
</div><p>As simple as the equation is also our implementation of SGD:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">SGD</span><span class="p">(</span><span class="n">OptimizerTemplate</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="n">p_update</span> <span class="o">=</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">p</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p_update</span><span class="p">)</span>  <span class="c1"># In-place update =&gt; saves memory and does not create computation graph</span>
</pre></div>
</div>
</div>
<p>In the lecture, we also have discussed the concept of momentum which replaces the gradient in the update by an exponential average of all past gradients including the current one:</p>
<div class="math">
<p><img src="../../_images/math/1a19014c123f1ac5481dcf78574684d63f35691e.png" alt="\begin{split}
    m^{(t)} &amp; = \beta_1 m^{(t-1)} + (1 - \beta_1)\cdot g^{(t)}\\
    w^{(t)} &amp; = w^{(t-1)} - \eta \cdot m^{(t)}\\
\end{split}"/></p>
</div><p>Let’s also implement it below:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">SGDMomentum</span><span class="p">(</span><span class="n">OptimizerTemplate</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>  <span class="c1"># Corresponds to beta_1 in the equation above</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_momentum</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">}</span>  <span class="c1"># Dict to store m_t</span>

    <span class="k">def</span> <span class="nf">update_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_momentum</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_momentum</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
        <span class="n">p_update</span> <span class="o">=</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_momentum</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
        <span class="n">p</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p_update</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Finally, we arrive at Adam. Adam combines the idea of momentum with an adaptive learning rate, which is based on an exponential average of the squared gradients, i.e. the gradients norm. Furthermore, we add a bias correction for the momentum and adaptive learning rate for the first iterations:</p>
<div class="math">
<p><img src="../../_images/math/4588bc6697481092db44b5d1f37b540b9f473b50.png" alt="\begin{split}
    m^{(t)} &amp; = \beta_1 m^{(t-1)} + (1 - \beta_1)\cdot g^{(t)}\\
    v^{(t)} &amp; = \beta_2 v^{(t-1)} + (1 - \beta_2)\cdot \left(g^{(t)}\right)^2\\
    \hat{m}^{(t)} &amp; = \frac{m^{(t)}}{1-\beta^{t}_1}, \hat{v}^{(t)} = \frac{v^{(t)}}{1-\beta^{t}_2}\\
    w^{(t)} &amp; = w^{(t-1)} - \frac{\eta}{\sqrt{v^{(t)}} + \epsilon}\circ \hat{m}^{(t)}\\
\end{split}"/></p>
</div><p>Epsilon is a small constant used to improve numerical stability for very small gradient norms. Remember that the adaptive learning rate does not replace the learning rate hyperparameter <img class="math" src="../../_images/math/97db043c7cba573ac389b71add78b048077e8a13.png" alt="\eta"/>, but rather acts as an extra factor and ensures that the gradients of various parameters have a similar norm.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">OptimizerTemplate</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">=</span> <span class="n">beta1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">=</span> <span class="n">beta2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_step</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">}</span>  <span class="c1"># Remembers &quot;t&quot; for each parameter for bias correction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_momentum</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_2nd_momentum</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">update_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_step</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">param_momentum</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_momentum</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_2nd_momentum</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_2nd_momentum</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

        <span class="n">bias_correction_1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_step</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
        <span class="n">bias_correction_2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_step</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

        <span class="n">p_2nd_mom</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_2nd_momentum</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">/</span> <span class="n">bias_correction_2</span>
        <span class="n">p_mom</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_momentum</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">/</span> <span class="n">bias_correction_1</span>
        <span class="n">p_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_2nd_mom</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
        <span class="n">p_update</span> <span class="o">=</span> <span class="o">-</span><span class="n">p_lr</span> <span class="o">*</span> <span class="n">p_mom</span>

        <span class="n">p</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p_update</span><span class="p">)</span>
</pre></div>
</div>
</div>
<section id="Comparing-optimizers-on-model-training">
<h3>Comparing optimizers on model training<a class="headerlink" href="#Comparing-optimizers-on-model-training" title="Permalink to this headline">¶</a></h3>
<p>After we have implemented three optimizers (SGD, SGD with momentum, and Adam), we can start to analyze and compare them. First, we test them on how well they can optimize a neural network on the FashionMNIST dataset. We use again our linear network, this time with a ReLU activation and the kaiming initialization, which we have found before to work well for ReLU-based networks. Note that the model is over-parameterized for this task, and we can achieve similar performance with a much smaller
network (for example <code class="docutils literal notranslate"><span class="pre">100,100,100</span></code>). However, our main interest is in how well the optimizer can train <em>deep</em> neural networks, hence the over-parameterization.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">base_model</span> <span class="o">=</span> <span class="n">BaseNetwork</span><span class="p">(</span><span class="n">act_fn</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">hidden_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span>
<span class="n">kaiming_init</span><span class="p">(</span><span class="n">base_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>For a fair comparison, we train the exact same model with the same seed with the three optimizers below. Feel free to change the hyperparameters if you want (however, you have to train your own model then).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">SGD_model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">base_model</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">SGD_results</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
    <span class="n">SGD_model</span><span class="p">,</span> <span class="s2">&quot;FashionMNIST_SGD&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">params</span><span class="p">:</span> <span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">),</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model file of &#34;FashionMNIST_SGD&#34; already exists. Skipping training...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_56_1.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_56_1.svg" /></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
============= Test accuracy: 89.09% ==============

</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">SGDMom_model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">base_model</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">SGDMom_results</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
    <span class="n">SGDMom_model</span><span class="p">,</span>
    <span class="s2">&quot;FashionMNIST_SGDMom&quot;</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">params</span><span class="p">:</span> <span class="n">SGDMomentum</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">),</span>
    <span class="n">max_epochs</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model file of &#34;FashionMNIST_SGDMom&#34; already exists. Skipping training...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_57_1.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_57_1.svg" /></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
============= Test accuracy: 88.83% ==============

</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">Adam_model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">base_model</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">Adam_results</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
    <span class="n">Adam_model</span><span class="p">,</span> <span class="s2">&quot;FashionMNIST_Adam&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">params</span><span class="p">:</span> <span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">),</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model file of &#34;FashionMNIST_Adam&#34; already exists. Skipping training...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_58_1.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_58_1.svg" /></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
============= Test accuracy: 89.46% ==============

</pre></div></div>
</div>
<p>The result is that all optimizers perform similarly well with the given model. The differences are too small to find any significant conclusion. However, keep in mind that this can also be attributed to the initialization we chose. When changing the initialization to worse (e.g. constant initialization), Adam usually shows to be more robust because of its adaptive learning rate. To show the specific benefits of the optimizers, we will continue to look at some possible loss surfaces in which
momentum and adaptive learning rate are crucial.</p>
</section>
<section id="Pathological-curvatures">
<h3>Pathological curvatures<a class="headerlink" href="#Pathological-curvatures" title="Permalink to this headline">¶</a></h3>
<p>A pathological curvature is a type of surface that is similar to ravines and is particularly tricky for plain SGD optimization. In words, pathological curvatures typically have a steep gradient in one direction with an optimum at the center, while in a second direction we have a slower gradient towards a (global) optimum. Let’s first create an example surface of this and visualize it:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">pathological_curve_loss</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">):</span>
    <span class="c1"># Example of a pathological curvature. There are many more possible, feel free to experiment here!</span>
    <span class="n">x1_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span>
    <span class="n">x2_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x1_loss</span> <span class="o">+</span> <span class="n">x2_loss</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">plot_curve</span><span class="p">(</span>
    <span class="n">curve_fn</span><span class="p">,</span> <span class="n">x_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">y_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">plot_3d</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">viridis</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Pathological curvature&quot;</span>
<span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s2">&quot;3d&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">plot_3d</span> <span class="k">else</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="n">x_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_range</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="mf">100.0</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="n">y_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_range</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="mf">100.0</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">curve_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">z</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">plot_3d</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#000&quot;</span><span class="p">,</span> <span class="n">antialiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">T</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="n">x_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w_1$&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w_2$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ax</span>


<span class="n">sns</span><span class="o">.</span><span class="n">reset_orig</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plot_curve</span><span class="p">(</span><span class="n">pathological_curve_loss</span><span class="p">,</span> <span class="n">plot_3d</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/tmp/ipykernel_1102/1102210584.py:5: MatplotlibDeprecationWarning: Calling gca() with keyword arguments was deprecated in Matplotlib 3.4. Starting two minor releases later, gca() will take no keyword arguments. The gca() function should only be used to get the current axes, or if no axes exist, create new axes with default keyword arguments. To create a new axes with non-default arguments, use plt.axes() or plt.subplot().
  ax = fig.gca(projection=&#34;3d&#34;) if plot_3d else fig.gca()
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_62_1.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_62_1.svg" /></div>
</div>
<p>In terms of optimization, you can image that <img class="math" src="../../_images/math/ac646631be98dce6e838bff770e0ead3e6113e94.png" alt="w_1"/> and <img class="math" src="../../_images/math/e9cb5884a68891fdac34b35f05e00cb70d7e73a7.png" alt="w_2"/> are weight parameters, and the curvature represents the loss surface over the space of <img class="math" src="../../_images/math/ac646631be98dce6e838bff770e0ead3e6113e94.png" alt="w_1"/> and <img class="math" src="../../_images/math/e9cb5884a68891fdac34b35f05e00cb70d7e73a7.png" alt="w_2"/>. Note that in typical networks, we have many, many more parameters than two, and such curvatures can occur in multi-dimensional spaces as well.</p>
<p>Ideally, our optimization algorithm would find the center of the ravine and focuses on optimizing the parameters towards the direction of <img class="math" src="../../_images/math/e9cb5884a68891fdac34b35f05e00cb70d7e73a7.png" alt="w_2"/>. However, if we encounter a point along the ridges, the gradient is much greater in <img class="math" src="../../_images/math/ac646631be98dce6e838bff770e0ead3e6113e94.png" alt="w_1"/> than <img class="math" src="../../_images/math/e9cb5884a68891fdac34b35f05e00cb70d7e73a7.png" alt="w_2"/>, and we might end up jumping from one side to the other. Due to the large gradients, we would have to reduce our learning rate slowing down learning significantly.</p>
<p>To test our algorithms, we can implement a simple function to train two parameters on such a surface:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">train_curve</span><span class="p">(</span><span class="n">optimizer_func</span><span class="p">,</span> <span class="n">curve_func</span><span class="o">=</span><span class="n">pathological_curve_loss</span><span class="p">,</span> <span class="n">num_updates</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        optimizer_func: Constructor of the optimizer to use. Should only take a parameter list</span>
<span class="sd">        curve_func: Loss function (e.g. pathological curvature)</span>
<span class="sd">        num_updates: Number of updates/steps to take when optimizing</span>
<span class="sd">        init: Initial values of parameters. Must be a list/tuple with two elements representing w_1 and w_2</span>
<span class="sd">    Returns:</span>
<span class="sd">        Numpy array of shape [num_updates, 3] with [t,:2] being the parameter values at step t, and [t,2] the loss at t.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">init</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="n">optimizer_func</span><span class="p">([</span><span class="n">weights</span><span class="p">])</span>

    <span class="n">list_points</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_updates</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">curve_func</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">list_points</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">weights</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">loss</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">list_points</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">points</span>
</pre></div>
</div>
</div>
<p>Next, let’s apply the different optimizers on our curvature. Note that we set a much higher learning rate for the optimization algorithms as you would in a standard neural network. This is because we only have 2 parameters instead of tens of thousands or even millions.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[32]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">SGD_points</span> <span class="o">=</span> <span class="n">train_curve</span><span class="p">(</span><span class="k">lambda</span> <span class="n">params</span><span class="p">:</span> <span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
<span class="n">SGDMom_points</span> <span class="o">=</span> <span class="n">train_curve</span><span class="p">(</span><span class="k">lambda</span> <span class="n">params</span><span class="p">:</span> <span class="n">SGDMomentum</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">))</span>
<span class="n">Adam_points</span> <span class="o">=</span> <span class="n">train_curve</span><span class="p">(</span><span class="k">lambda</span> <span class="n">params</span><span class="p">:</span> <span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>To understand best how the different algorithms worked, we visualize the update step as a line plot through the loss surface. We will stick with a 2D representation for readability.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[33]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">all_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">SGD_points</span><span class="p">,</span> <span class="n">SGDMom_points</span><span class="p">,</span> <span class="n">Adam_points</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_curve</span><span class="p">(</span>
    <span class="n">pathological_curve_loss</span><span class="p">,</span>
    <span class="n">x_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">all_points</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">all_points</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">max</span><span class="p">()),</span>
    <span class="n">y_range</span><span class="o">=</span><span class="p">(</span><span class="n">all_points</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">all_points</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()),</span>
    <span class="n">plot_3d</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">SGD_points</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">SGD_points</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">SGDMom_points</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">SGDMom_points</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGDMom&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Adam_points</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Adam_points</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Adam&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_68_0.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_68_0.svg" /></div>
</div>
<p>We can clearly see that SGD is not able to find the center of the optimization curve and has a problem converging due to the steep gradients in <img class="math" src="../../_images/math/ac646631be98dce6e838bff770e0ead3e6113e94.png" alt="w_1"/>. In contrast, Adam and SGD with momentum nicely converge as the changing direction of <img class="math" src="../../_images/math/ac646631be98dce6e838bff770e0ead3e6113e94.png" alt="w_1"/> is canceling itself out. On such surfaces, it is crucial to use momentum.</p>
</section>
<section id="Steep-optima">
<h3>Steep optima<a class="headerlink" href="#Steep-optima" title="Permalink to this headline">¶</a></h3>
<p>A second type of challenging loss surfaces are steep optima. In those, we have a larger part of the surface having very small gradients while around the optimum, we have very large gradients. For instance, take the following loss surfaces:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[34]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">bivar_gaussian</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">x_mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">y_mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">x_sig</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">y_sig</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">norm</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_sig</span> <span class="o">*</span> <span class="n">y_sig</span><span class="p">)</span>
    <span class="n">x_exp</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="p">(</span><span class="n">w1</span> <span class="o">-</span> <span class="n">x_mean</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x_sig</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">y_exp</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="p">(</span><span class="n">w2</span> <span class="o">-</span> <span class="n">y_mean</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">y_sig</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">norm</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_exp</span> <span class="o">+</span> <span class="n">y_exp</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">comb_func</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="o">-</span><span class="n">bivar_gaussian</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">x_mean</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">y_mean</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">x_sig</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">y_sig</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">-=</span> <span class="n">bivar_gaussian</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">x_mean</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">y_mean</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">x_sig</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">y_sig</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">-=</span> <span class="n">bivar_gaussian</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">x_mean</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">y_mean</span><span class="o">=-</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">x_sig</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">y_sig</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z</span>


<span class="n">_</span> <span class="o">=</span> <span class="n">plot_curve</span><span class="p">(</span><span class="n">comb_func</span><span class="p">,</span> <span class="n">x_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">y_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">plot_3d</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Steep optima&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/tmp/ipykernel_1102/1102210584.py:5: MatplotlibDeprecationWarning: Calling gca() with keyword arguments was deprecated in Matplotlib 3.4. Starting two minor releases later, gca() will take no keyword arguments. The gca() function should only be used to get the current axes, or if no axes exist, create new axes with default keyword arguments. To create a new axes with non-default arguments, use plt.axes() or plt.subplot().
  ax = fig.gca(projection=&#34;3d&#34;) if plot_3d else fig.gca()
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_71_1.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_71_1.svg" /></div>
</div>
<p>Most of the loss surface has very little to no gradients. However, close to the optima, we have very steep gradients. To reach the minimum when starting in a region with lower gradients, we expect an adaptive learning rate to be crucial. To verify this hypothesis, we can run our three optimizers on the surface:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[35]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">SGD_points</span> <span class="o">=</span> <span class="n">train_curve</span><span class="p">(</span><span class="k">lambda</span> <span class="n">params</span><span class="p">:</span> <span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">comb_func</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">SGDMom_points</span> <span class="o">=</span> <span class="n">train_curve</span><span class="p">(</span><span class="k">lambda</span> <span class="n">params</span><span class="p">:</span> <span class="n">SGDMomentum</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">),</span> <span class="n">comb_func</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">Adam_points</span> <span class="o">=</span> <span class="n">train_curve</span><span class="p">(</span><span class="k">lambda</span> <span class="n">params</span><span class="p">:</span> <span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span> <span class="n">comb_func</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="n">all_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">SGD_points</span><span class="p">,</span> <span class="n">SGDMom_points</span><span class="p">,</span> <span class="n">Adam_points</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_curve</span><span class="p">(</span><span class="n">comb_func</span><span class="p">,</span> <span class="n">x_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">y_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">plot_3d</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Steep optima&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">SGD_points</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">SGD_points</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">SGDMom_points</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">SGDMom_points</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGDMom&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Adam_points</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Adam_points</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Adam&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_73_0.svg" src="../../_images/notebooks_course_UvA-DL_initialization-and-optimization_73_0.svg" /></div>
</div>
<p>SGD first takes very small steps until it touches the border of the optimum. First reaching a point around <img class="math" src="../../_images/math/271472e3187b21deba85013f22177c59267af6e1.png" alt="(-0.75,-0.5)"/>, the gradient direction has changed and pushes the parameters to <img class="math" src="../../_images/math/aa44f15c3652ef773f394b673e3c433e2e4ac67f.png" alt="(0.8,0.5)"/> from which SGD cannot recover anymore (only with many, many steps). A similar problem has SGD with momentum, only that it continues the direction of the touch of the optimum. The gradients from this time step are so much larger than any other point that the momentum <img class="math" src="../../_images/math/8587676bd7e72efd366bb5bec309d64dc297ccc1.png" alt="m_t"/> is
overpowered by it. Finally, Adam is able to converge in the optimum showing the importance of adaptive learning rates.</p>
</section>
<section id="What-optimizer-to-take">
<h3>What optimizer to take<a class="headerlink" href="#What-optimizer-to-take" title="Permalink to this headline">¶</a></h3>
<p>After seeing the results on optimization, what is our conclusion? Should we always use Adam and never look at SGD anymore? The short answer: no. There are many papers saying that in certain situations, SGD (with momentum) generalizes better where Adam often tends to overfit [5,6]. This is related to the idea of finding wider optima. For instance, see the illustration of different optima below (credit: <a class="reference external" href="https://arxiv.org/pdf/1609.04836.pdf">Keskar et al., 2017</a>):</p>
<center width="100%"><p><img alt="ff8b1cb3271841b5b42dd2e31fd92990" class="no-scaled-link" src="https://github.com/PyTorchLightning/lightning-tutorials/raw/main/course_UvA-DL/initialization-and-optimization/flat_vs_sharp_minima.svg" width="500px" /></p>
</center><p>The black line represents the training loss surface, while the dotted red line is the test loss. Finding sharp, narrow minima can be helpful for finding the minimal training loss. However, this doesn’t mean that it also minimizes the test loss as especially flat minima have shown to generalize better. You can imagine that the test dataset has a slightly shifted loss surface due to the different examples than in the training set. A small change can have a significant influence for sharp minima,
while flat minima are generally more robust to this change.</p>
<p>In the next tutorial, we will see that some network types can still be better optimized with SGD and learning rate scheduling than Adam. Nevertheless, Adam is the most commonly used optimizer in Deep Learning as it usually performs better than other optimizers, especially for deep networks.</p>
</section>
</section>
<section id="Conclusion">
<h2>Conclusion<a class="headerlink" href="#Conclusion" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial, we have looked at initialization and optimization techniques for neural networks. We have seen that a good initialization has to balance the preservation of the gradient variance as well as the activation variance. This can be achieved with the Xavier initialization for tanh-based networks, and the Kaiming initialization for ReLU-based networks. In optimization, concepts like momentum and adaptive learning rate can help with challenging loss surfaces but don’t guarantee an
increase in performance for neural networks.</p>
</section>
<section id="References">
<h2>References<a class="headerlink" href="#References" title="Permalink to this headline">¶</a></h2>
<p>[1] Glorot, Xavier, and Yoshua Bengio. “Understanding the difficulty of training deep feedforward neural networks.” Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010. <a class="reference external" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">link</a></p>
<p>[2] He, Kaiming, et al. “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.” Proceedings of the IEEE international conference on computer vision. 2015. <a class="reference external" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html">link</a></p>
<p>[3] Kingma, Diederik P. &amp; Ba, Jimmy. “Adam: A Method for Stochastic Optimization.” Proceedings of the third international conference for learning representations (ICLR). 2015. <a class="reference external" href="https://arxiv.org/abs/1412.6980">link</a></p>
<p>[4] Keskar, Nitish Shirish, et al. “On large-batch training for deep learning: Generalization gap and sharp minima.” Proceedings of the fifth international conference for learning representations (ICLR). 2017. <a class="reference external" href="https://arxiv.org/abs/1609.04836">link</a></p>
<p>[5] Wilson, Ashia C., et al. “The Marginal Value of Adaptive Gradient Methods in Machine Learning.” Advances in neural information processing systems. 2017. <a class="reference external" href="https://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning.pdf">link</a></p>
<p>[6] Ruder, Sebastian. “An overview of gradient descent optimization algorithms.” arXiv preprint. 2017. <a class="reference external" href="https://arxiv.org/abs/1609.04747">link</a></p>
</section>
<section id="Congratulations---Time-to-Join-the-Community!">
<h2>Congratulations - Time to Join the Community!<a class="headerlink" href="#Congratulations---Time-to-Join-the-Community!" title="Permalink to this headline">¶</a></h2>
<p>Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the Lightning movement, you can do so in the following ways!</p>
<section id="Star-Lightning-on-GitHub">
<h3>Star <a class="reference external" href="https://github.com/PyTorchLightning/pytorch-lightning">Lightning</a> on GitHub<a class="headerlink" href="#Star-Lightning-on-GitHub" title="Permalink to this headline">¶</a></h3>
<p>The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we’re building.</p>
</section>
<section id="Join-our-Slack!">
<h3>Join our <a class="reference external" href="https://join.slack.com/t/pytorch-lightning/shared_invite/zt-pw5v393p-qRaDgEk24~EjiZNBpSQFgQ">Slack</a>!<a class="headerlink" href="#Join-our-Slack!" title="Permalink to this headline">¶</a></h3>
<p>The best way to keep up to date on the latest advancements is to join our community! Make sure to introduce yourself and share your interests in <code class="docutils literal notranslate"><span class="pre">#general</span></code> channel</p>
</section>
<section id="Contributions-!">
<h3>Contributions !<a class="headerlink" href="#Contributions-!" title="Permalink to this headline">¶</a></h3>
<p>The best way to contribute to our community is to become a code contributor! At any time you can go to <a class="reference external" href="https://github.com/PyTorchLightning/pytorch-lightning">Lightning</a> or <a class="reference external" href="https://github.com/PyTorchLightning/lightning-bolts">Bolt</a> GitHub Issues page and filter for “good first issue”.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/PyTorchLightning/pytorch-lightning/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22">Lightning good first issue</a></p></li>
<li><p><a class="reference external" href="https://github.com/PyTorchLightning/lightning-bolts/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22">Bolt good first issue</a></p></li>
<li><p>You can also contribute your own notebooks with useful examples !</p></li>
</ul>
</section>
<section id="Great-thanks-from-the-entire-Pytorch-Lightning-Team-for-your-interest-!">
<h3>Great thanks from the entire Pytorch Lightning Team for your interest !<a class="headerlink" href="#Great-thanks-from-the-entire-Pytorch-Lightning-Team-for-your-interest-!" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="data:image/png;base64,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"><img alt="Pytorch Lightning" src="data:image/png;base64,H4sIAAAAAAACA9yc2ZajRpfv7/0UefKWQzFPXi6vlhCaQBIIBJJuvhVAMM+TBC/W9/1kB1VW2Wm7qo/dpPriy7VSQlLEn+DHjr13BAQ//fT4++X/LA6icVGll6BJk19/+uXx9pKAzP/8CrPXFxc0AHXyJK/QNHfh51fQNvnXr5PQDxq0CWA6fv/lw9cfXFDF375/bL/++tPLyy8BBO5jY9xMYQNenABUNWw+v7aNh/JvZZIwi18qmIz1shotKujBxgleX4Jx6/Nr0DRF/TOG+WETtPantzdQjyL1JydP/4kE6MaGVvVXjbaGlZNnDcyafyr0JoA6Sd66n2rqE0jBkGfg9o9b9GgDGqbAhz9oFfZntVFp/DmDTvN3Ab04VV7XeRX6Yfa3xf5/qMa/b0rv5EdLyfKsT/O2fn1JoRuC8ZskeX0Jx7p+FTb959c6AAxBoucu4UqXvS5LFswrM9wVF4Q7yUeFE7Thfr+siLiXlBXE6jVdmxQU45pfLM53QFl5MhMdWQUOTcKy8iCb4hEY+tlG1T5/fn07srrpE1gHEP5dTNjbJuZVIIW3vIprlIGewNlAcKEgECyAAsBZm2FJihRYCkL3k1OPh4k9mL5Z+AQclTPPi3V1qwvtcCzOphIG151KkvXy1m5yylOIRb/MGXC+WZnJLAcTy5LZKjtjFOtthJVmOqS553bhxqDo4068lXwEU386DhsGoAvzqkaB6+AsD2gOeLhNCjQOaVZgXIaEJM8IFP6Oxlcev79MIcM727lPqGUp+FIT3XwfSF1XemaI4PweT7q23QcHpVscafd8k9z9SjDmKbF9mEwYdf14FFs2Jgw+P9VsOFeXqy6mlQ8g89UHeIRD0gRPMRQEwHZ5nLZZyiFxngAE6dnwNyyP/lI7VVg0PwThjk6iGh3G4+17KBbrYg8QLdbASYwJGIWIrq0WtQz4OUs10GAyWzYozNAo3iADaeEZwlw4uG56kp1dpvErIIlrKdk727hYBRsjiOv+PHugaPri4eaLIgkd0IR5hkWjB3hr7utLXTl/GwvMurDKs3R0FSjucIB2WftTVL/++gv2Jvc1GkxE4fMa7fDO2g8pWU23XI4Yh3A/5xo1LJElspgFrtFZt6NTUVm5dz1xvQl42zlbp/SIEIaibim8VZyTppRAGt0vm8sX8vaxKJygzWL0nUPhKYemeI56Ao+lfFaJa4ThrJRkJ9LnmhOGbbEw0bGQ69rmOgut7bFiBH1uyVWWruWNvL+pBhI2vCttXLAaDGqRD71kaxq8nqSyc3YfbBpvPDqYuWNiQdCAcTyX+QuLD+gn27lRYIeZRhj9wcbXYXRTLbWIDLpPTnK/ZXa+tCXnirlR4CaXmHZ3W7UC2S3FYdev0qtrHVf+TF679ALBSqk2z02twg+G8btjHd0HDQXP+R6J6Szmp4Vdna+2LGh5dao8XM4l+zrbBkY2IPNylxK4XV5Fmb0x61vPXcDhqNFLenEOlHhmGh3YBvvgfodnP/W3ioobc844/I2O8iUnHLPHNoFo6H5+/fT19AO3A5kzBs+o/lron5vQNw0UZ2jctYH7hA7FNJasV1wazAY9oPU7czkufdlr3NVdvq3l7LC63JhFco/WQayZp+NleaLmMbPZHVY7uZ2FWyMRqcP5sqPuw8rpo6GVlpo2gVs2pokNRPMMHYvmSTIJ4J/FUMi6zBjcwRNI4rtrXt2o/JzJPQOhftOE+6qST3Rw16B4vqf9PcjgZr0PFpuy5Tsr4NN1XQvLROSYs4vZqnVfLsTOPeQM12TGkWYz+2/0xh+RdMYhTRpWVV5NQfi7CuriDjtmRt4T2NkBHFp23vcct+mDS5FrOMtRPn/Ny2o+xynKRsC2FwYuJGnkzOk7LL7RNIIctJV+tPqhxGItZvkYyY7Cujo5xzOvGf4Udt8GhfU0eL/JoCwkOJuyn5EkcMT62hIGs7XK434p3KqU3J+vs/JidsP9yi+6vGJOTIIprTS3V+L8zHB61rP4MULK0/U666T5apns64HghIWUkevlzTcnWV42jgKbcBq6Nw0UegzhCk/hppLCCU/2aXCKxrzWrESrYJ3BDfbFrlly+N2UPXo1ojFlMS9Xzb3A6/422/EYzSKhiMfuMjx2O4KS5t0iK9m8t463cELMeAw9q9BuHxVqtC5Cdxw1+xUogokkf6CKUh7rMbRLPoEtqLxkt3TMwZxdgEpmXMr5caKKgVWbtbhojiroSNFm7VKBW48vfDuPrC0l1VA+bxRxU3r3BOtYUM3d9iroSlrspOsEti5MQD+G0iQHbpj5KEzgI3efAvYHkigLbA4y40jx46myEZbrSyS2LaSHzV7tNixPm8OsFYqh9VkzIgOEagnFFdtjcKSjrA3XVw9qclJT2d0xMJ1aI/mm4ZZN6EsYgHmE+BP8pFsBf3zJi0kcv4mgEFAe5Gn+CeRMXXeRyxCS4U5HitC+sMECs9y+uM8kN7eZJXncyfdWdmZrnZSK5WWrQuOcm/cG3599nGq8iHRjQKZSs9aRLiouOlhMsEfohg0a5HmM1tCpYPMRBvkjTZRhSFpwPeEJXKWqvC6T0aKGja3k13V380rhGuXMkCMXYkYTeAbVxaqTEcs0TJXAw73h5aV7OOjXvd4TfqPIG2ux2YQHk8KTih1OW3FC7HkwmMyQIG3wmAt7hl8My91mtbqucn6m7SIn36SyujPqq2keEzZoRP3CzYtjWqzUFTZbIlaJqNghwtwmsLTDJhYccb9TJVBzB3KYHdm8zPBmCq80j0K0CJ14jAofYYPf0Rs9Igl4h3yGRyzLo99cIGLOoZBrnWxc9JnQmjZVinIg7RKqXrt1pvIz2zx6bRrph9Y3RGlPXJQ0y8L9fHe2Oa7pyZvr2okt60CTiAnjFw+EiZ1PQvhVAgUA0Cxu/3XqYDq1usfl+ZBQ66qFNySvtgQm0U7fnsJ7Y+KtjlO6vi4uxyGYwcKCUev3ak3NbutCQ6SUWYOd5xI7dcB2NHKXwNLWHMafYIUeBE1bQdQBSZK3H+IMfyCJ2hTp4gDnnkA14JVjqXhu58okFutyxG8vPRv1czcx+ttdoZPyZrIH8+BSuqge/dOWxipKXi+1q9pebviKm1lWsL1gmzxsLko3s5aQmWKLYdI8rrtkRTsN5TsdlOcc0iXAM2I0PcjFjiezkytujc7ucVLN187eQYL9MSjp7Zi5eDI1024ZK4TZVq+S+27LqGVJqn1HFqdUKXxqI82vnijW9SaO1v0tmsDPH0NoBYv8I8zxz1oopCgSCNQzJkmDRWjMrvvEjcLTXo0ZuG+CYQys5oXhlstyeUIycsFZx0vvD+LdXjS3g3BYyItaOt8vNh/PdhtudJI8ph0p99ysTdH0nQm9O8zqxwXUGi1bWPVTKP5RCeVpihconHgCQ32uXM4a04hXrHBcwnfu1ZIGRlvZrBqy4BwiMb+9Sdyw2Dv4MTWymXpyNRzQmCPEDV9vgmPq8Kf+grmLAaEqIq+ZvzPR/GOGXdh8KT5pRuKdDErzBOnyzDNmFePNEcIzhme13EvLPEtBrzqnZJMmR5fCI4FliPC0rFfNfomZBn4okSyN7DOClNRci7O2SpRC4HMl1OzF1SJ1araK9QnjlKhNC7TJp5D7KoEKOE8zkHtGLm02LrjtsINet4jLdiRzvbAnRwPYaqXM9EaX9KguXel2R2DP86v9ZRVrJJ0j2+Uyp5MLwt2U3dHjgFgp93jG6GU7z/cT/F8C2swJvkwBfoQL/I4cyrAuxwDnGZnh8cicYkSWhB0m1qXipP4RioHOXRdRyvt355AtLpYV71xo1FVhYfdFsl1rkn/yWklk19uz16t7x0LyubEud1nrjLyNKTTDbMxGAlBNg/ibCkozBGRo4RkRpDBrb+ZqQRrG2KIYrnVpkLkqB+zmsGiFY3xxbkbrpQi/wU2DG8ij2UTRqkJO+6EkZtZubVSafuKJBd+zB8M1vAEv1AneL4VNFTrjsDaBzqPSR5jjjzRRwDA24T0lqnDBFaeIzA+XFHtb1Z6T971oGeXtco/OSAkNpRFNTtwimwNyG+JBTIljfqSswXIGz0pvyr6skHVLxdrBj+dgldFZY02wySxvQu9rhdG06gb1cqedFGF+IIlCSLAc7T7j6sEqa6KVd7jtzvX+MRC5p4NB4awrspQi4/yhZNbVpR3msNpY5AGbYwNfru1osUmXpzl9HVo+7mSTuJ+EeXriRKAocA8nRJuigmnYpmjVZhmsJtH8kxRKAM5mKeIZ0ScHMrNYHygk0bCkLQJbtrfJpXU2CHFzbcKJB2UVEAW1jrJgVzSnVcUt093+Zs5PK6WI1/3G7iDiXdqzIDbEbZDXCAumUaxh5kD02z1a0zD+QQsFBCBtAf/rxejpHFOAHw49lUW4Qzjl/I7PYyfdLGuldPQrstu0sXYfk6IxzlvGcC1WiLbzD7mlztbXgaZ2rNdHs9OKiNasg7GKGoayKvYTZhqLKh8HcBAtxuT5I/zm9/RQQQAuDqlnXKUJ/FyXFSNR+A21sSrMUA578YTs6dgVG7dai1XVuvReDvxjolTGDT+f1KCcg4V7PYVLfVhoxxuWLTUivpEmNFLcF5BkQiz6evwfgZBncUCQ5DPG0hSHIYU81IHcIeQ1nFub4HpYVTNkdpZzHqNa4aCX3JWBi9ksrKQ6DYYYlDQkTy5Fr00YYf0+3ziN6B/YRR0cNiJfHidEmqJN6rH7tU3wuBfk7fLTRxjjj2VR17M9DwL6CXRvmx0+eHIMmjuxvSQx4XhSIu/m4eJ4WM50V3I436s210xd85wEBCjcth6dICtziFjb4o9J5p92hTIobjqoGseemwZM6OMVBG4Kx+GJ8xFQ/6qGOjhPUbTzjLHiYhjcY2Dkvd4e7+v4khgbi/aUM4X1F9ZMzNhHYEX2cp+ddDtUSuMYhUiTkMJpGXrOUjft2No35QlRhy1jazPaBP5uQtypoPc1KZx258R7HRT3KMd16WdcUVhgu/s8ao5qBGt20WWXVUi60pqbaVKmdHpnHBbAlmI9R4hBZXVduhY2ny8ZMl5a0nEmcA2hEIK3xFTKzujC39Q7tZ3gH8dQW+RZHXYQbTMXVo8RSwa6aSi/KzlS9SiKwJ9BVbd6itVJflsj2GWou4ufFshGckW8bHbO3GPrS3A2JCciwFIVFfVgHLKDXA7LcN+vyXxvLlykZbSQh+Vls9c3Lckrywn+8y0BHD1c3haTUqE/CKG0wDo2B5+RT+qmK+MyK6iZRi/FwC0SlsEPR6Qrkot1Ll33vo+llRxoHtvEMCg7OV5KPqNo+tUXTbkhG88RxMzaJqks3mzDCAspmtCva1iFsEYbYE8L3u91Rn4MR4+pzxP4CbPS2GsSXot7AWsLEh86z3X2zZZdXsy2XKkGGxWeyR87Rj8C/Vxx63u1EvGzdHauOOgS0Wsv2z0+j84ydzUE/GLi20n8Ovho4OOKitMm4OHWPiDY/DeyqEfjjynyZ1wBizjVJtYIeW7oy0bWq62inO8Jtt/HcWFhGKMqRtEy/Ha14IvyqB4Wi2grl9GBvahXF0Hmc9rctvZajYau3Z0DCxMip5zgNcdjeLOobzfMTmL6ZzGU92zKtd1n5EKZDKiTOyMPnAnpbUbSPpiTeH/ytp3O3ZLM5TVeREJrwYdkc1Z8IMN0YcUKhHeouVWNgEzgl2LCeIewx7dIOGRIM8FTNkEF4ae3H6ZAfK+DCg5gbY55xpwQXzbGITHDebods9liS4kYws1sitK5nco0M2njra8Kt6VThAr9xlzNJLrier8nqvkxwhxnxW9aohDljdzNvNXFHPQimcIvnHQ3z1gd9UhgMxT1jLtsLZk3zzcq8U4aZh4tfEh8WQ2c3alc462RzTF5gOpJda+BmGLr4u6dz2vZWHURI+IHeOeQwd7sSEkcTordGKJBtz7VT/CKTQ7qSf7viwDK8MAjGOYZs2NEp1+7W1GNeVLN3yJ9v9wvV2a6NEiJvkQbcs/H7LYMdZvkV6vLLpa2p6CDeZN2cxy/0rZ23bHE1e8s5wzuxIna0byvTLGvGxyPPnfDNp3E7XcZ1GU9WhDsZ9DzOudwuIBCvKvx4qhZA2lpQ1UqbHUMzjzfr60lz3m2cM56clnSSRXihz0wlwp3XuhGaPQtfTAKhLfy3tuZhb86AWBOsLc262A46ab4NwWUgx6HQ+oZmd/psNxb2N2591mRmJ7IXyhPc5fLENRp619Op/WJ3gv4iq/L+XWlEP2R4vrIBPHd5LGlybhwPlPuwkHbzfPQBhh1WK8WE2Lrl9WndQOatkbr1k6n3SH2VzWUwSFBu8wzZr8cucWQM+cbiGWu6fosN7hPrTjO9jTP8W9tdTzYO8Rt+HOfqKkAJWuDYMCSAxPYrFlf2q5rN8zcqEL5HJy97W3MFN0JvfcGbT9Bb6CaFCR+V0E5HNi29517Ez9ggdR1I/m6p29Ipnf6+uzci5ZS15Jm99GhK51qFxXHa507GjZjlztdvGOKeFB8SglPfasd1NwnzZtFCQKkk2u72fOJp1z+Brx/AuRxU0gdjlnvY1DBPq7J4/SfYUzlUO2HJo3TxQpxwgzH7+xl25NbS78XIn5yi/GcBH0VuVV1VzeHi5/DE4c1w1LtBMLeXzNWzAeLRVRJ3h0Xp0Bb+3Bh5x/MwQ09r0aB4zrQFf6cOfz0bV19Bh7r7rsQ3ooxx319+bpa+/PrLXSb4LMLu9CB6JcPr9+MqAmbBP5a9CNjJ3hb2J+FmY8luZ9/KjL/BTQv6RiEYfXyX//5ovbGo5zyW7G/VHyUWoXNurV/wd603y38f2ugC99aPgJ510YjgC9fVG7w8fptVy+3agQ47t3Lq5dg/AUdP4zb6WPx18ts8/K4eAHGgp9e9HGQBF/6vK1eHqtKkvr/vmR589KMwnYeJrAqxgEU/PSCvnzc4b7++m5t89ta4i+t+d7JzwuYvf36jgByT5Nvi47flfj05esvAD+/vvH880MAvKQNXTR0HhS/u2j5YT9fCj3KPA7uu4JfTkxRjbuuHp3Cs38e2/yv0H13aggaJ2ieZwWKpliG5N9VfDujj6McPWHTZmM3HT+8qzt+erhIguY4imMFnPltefh7geYWNiPzn788BeHnsWe8U/h2VL/5gv6/f1gCwfEsyXIkjnEC4Qo0jz9uWYYoQUAb5RnAoQLP8ZyNEwSHu4/mfK8l457gu0b8x9u+flTaAdV7YHWbpqDq/5WAyof/+tLaH9X8ckbeVf1A0/zBHv+tut8fjel3M879n79i/1+3o7824meQNP8WqL9zgI9+8q+Hgb07wK/u5fsVHn7xXdncjr48eeT7Zf8XOscfdthWyXcs5p0//R8r/9v1ui8PfxlziD8Gpbd85e8+ReMtCxn//x8AAAD//wBkApv9PG1ldGEgbmFtZT0icmVxdWVzdC1pZCIgY29udGVudD0iMDgwNTozRUIxOjE0Mzc4NToyNjBGMDY6NjEzMTFBQkQiIGRhdGEtcGpheC10cmFuc2llbnQ9InRydWUiLz48bWV0YSBuYW1lPSJodG1sLXNhZmUtbm9uY2UiIGNvbnRlbnQ9IjgwZTI3Y2NlNTMwZmI3NmViZmUyZGFkMWMxOTU1MTNkYzE1NTk4YjQ0MmIyNjBmYThmNTYwNWE4NjhkYzYwYWMiIGRhdGEtcGpheC10cmFuc2llbnQ9InRydWUiLz48bWV0YSBuYW1lPSJ2aXNpdG9yLXBheWxvYWQiIGNvbnRlbnQ9ImV5SnlaV1psY25KbGNpSTZJaUlzSW5KbGNYVmxjM1JmYVdRaU9pSXdPREExT2pORlFqRTZNVFF6TnpnMU9qSTJNRVl3TmpvMk1UTXhNVUZDUkNJc0luWnBjMmwwYjNKZmFXUWlPaUkxTVRReE5qZ3dNVEF4TURBMk5UZ3dOREV6SWl3aWNtVm5hVzl1WDJWa1oyVWlPaUpwWVdRaUxDSnlaV2RwYjI1ZmNtVnVaR1Z5SWpvaWFXRmtJbjA9IiBkYXRhLXBqYXgtdHJhbnNpZW50PSJ0cnVlIi8+PG1ldGEgbmFtZT0idmlzaXRvci1obWFjIiBjb250ZW50PSJiN2FjNGZkMzNiNTdkNGJiYjllM2FlOTMwNjVjMzYyOGNhNDAzNjVkYTFiMWI3YzcwMTUxYWQ0ODM3ZmQzYjQ3IiBkYXRhLXBqYXgtdHJhbnNpZW50PSJ0cnVlIi8+7D1pd9s4kt/zK7jq7d7d16HE+0hsz3PSzjGda2P3sT0zjwuSkMSYItk8bCv9+r9vFUCKhySTkuUcszOdMUkQqCoU6kIRgB48EOB/RwuaEyEiC3o8msdXNPVI6otZ4X6gXi7mZDYSvDjKaZQfj1KaxFmQx+nykWxahmKYijQSfJITMflAbsQ8JVEWQNWTBw8etEHPgnxeuOIlXboxQzCP09wr8mwj+IdZXKQeFb3YpxsRHI/ytIBXkxNE1EWW0RCIp74YBtHlSLgiYUE5eIcD3kZ0lyGzOJ6FVASiqAisCaaBR/IgjhpEe/Jl8YP4o/Li5c/PDFUPl4mX/fIu/kG7DMJYFz+cOtdPni0vtOL1/4xOukzpB//jhT7LrLl0fUXI7NWPpz//8rvluvTNdfTbbz/Kqfzrfy+zX9WbIn310z7gf/s4/3l59uzaNa9VKonF24swX1h/zbxL5Zl+nv8czH+Q6M21Ms1O9wH//NdM/zH+6afLN09jcvpbZF6/eyPmkvxueZ3Yr9Xs7EOUO6rz2y/vvNFJdxRjL4/DZR54mTiPs7zJ8zjEAY7TMRcrkiRjL16gOGwBADXEwG+A4A1vaUGvoJ5YpGGj0TzPk+zRZLIF/6QUcnqT0zQi4cRN4+uMpg6DdQuufbBUrysdaIImgJxDDuO1MZl8F+aPCyBLxMrfzfLHrAS1oy5xw9idgI5eD9K+tZFL8mARfKThUsTW0yCkDQL++O73Is4fg8hkQBl/eCTwq8YvD8vHFPoYg5GoKv3tH9WbfJlQ/7TwgRyPbnhPojhaIg0v31UvkebqdZLGaN9e+m30smGqpmlIsim1CbkiaUDccBOmKSV5kdJnIZlteEtvElCNBXS8flkyIMuhYdYm4H0RRUE0ayMnZTdf+hsQMMpwiNfg30Jz0Om3ImmqZahgv5Q2arDX7Zo4ijAq/OHPh8JeuCzN1k3VNntw5SmwFllXYrsNpGkriqkpWg/IGWhkPndmc+o5ccRcEbDb4aOEPG63D8mSpl0hAWyGaWmWbuvt2qAfUzCCp2Gldd0Rga4E+XIDvM0MoZH/dvqeRDNaNZAlSaqZvhXc5rFcB6cfFpwxDNzQzlqHBYe8k2oZmsYQB/g/r2nPH3/WOL8oDdUlS1EVXVY/gYbqqgrKJFl6D64dNFSXFV2VLVsepqFJEYZOSn8vaJY7xGPsc8BqL5J8kIrqim0piiJ1hHZvFd3CkX11astg/hNKrabqpqJqfZJ0EKmVJcs0TP2AfkWXLUk1Fb2P/FJqgywr6H7iquiKpnS1Y39x3cyKvcV18yjeRVz/0ZXCNTkDkfCDVjN+/dvf+TVO/16R80d5tyC5N/97Vb28oTcwIKuq5Q0Gqt2K/w5xq+MXi8XSIXmeBm6R0247jDu77bwiy+PF9jZs/nkLsub7P//RZnFXJJs0loxrN2AhfKvJ2zoaf04jmhKYGQtV6CzAEAlPiHd5DaFQJjwFiYWhcoMQ5KCrHiDkRbI2TEkcBl5HHWCi4MeLQ8myYum6rur2YWRZMyVZtWVb6wPXkeWvJpK3bNNSTKPDrvuJ5A3AJsndoVnHRUKvCAnMZftN7pbxWYOZpIGHsXsXdl/sDvGiaquqcSDp3MLuvaVzM0f3t7T8RlYMS7ZU3dDHsgFhjApeTbpFHiqwsmEhRZZuYjvLkmyIhHrG9lZ/qhqyCYSoXTlYGc814e8CAZIUBYRENfucMiY5nKp5zfo1gKasqTDnV/Q+kcsSAoa3F54lm7phSKrUpxZxOiNR8JGNmJOEJOoHbcmqBOLRzU+sgQ4yJ4xnM+o7QT9UWzJVC0IdpY/gGY37gCkS9F4zFFXvi/TL6J76T4s0Bau45m/cOH8WhDlY3cqOPhKmJMxWWRzieXERDc/isDzcmoDVhn2jWV4XF1uGIZY1ra+D86Wfxo4H7vFy7JNsztIN45wSbw6Cmcdx6MY3jpeTLk93pciSFU2GXkt9c8OscBdBPm7KXYY5xSSkOXWyYBY5RXJ3ajTdshXV6ovDo9hZUFB7bwzW17sEWcVcX+BTJ546dQ7x7vSYlgkRhGH10MNHCpjTTA3BWHmXYZDlY+L7DmZJ706PJUE8o/TKTzla9WcRJ1gkcZrDkOEMhh6AEEPG6c0wxsxpmIxDStLIWcQpTHNcGC6nQR2jauU670SXpauSrPUZuRyTA0ypx37sRHHugMkHg4czL7/wDkMLhCKGamh9voHziMKEb+mgCLGvTaDbY5Y1n4LC15w6AFG6ISumNWzgNhDFBemQJCmyZGu6ZveGgrcrWRBdBWCKFnTh0jQ7AFWyZICq7c0ormmHZRRMpUHC78ioIvGRMJB0/Lxyd6oUjIHM3oxfD1V+kC2C7ADjpmqWqiiW0UPOBofG0i9X7Hbl2u5Oj66o6NKGO9iGyBzKXCs6uDFZV/v8aoMIrkcO06qSJYeixdYVRTH6TPSmASplthThO5OiGxoE24bSJ7pXAb0eeyHJsjSOFw5fJOA0KTsAKeBRIQLuE1uuRQ1aMPICv8XK706FCaMDs5m+sekwpM4c3Qm3pkgwmRhmR2rkpZ097GhYOOHU5F1puRfJsGzNUIE3+/FlVXB3QmxZVVQIaPYjZBqkEHMdlBwDbKvSFwt3yZlBVIUfhjx6AG8Djk9WZW1XC7JbvMtv1j5V30KWpcoGTBV6nU6RADd8/O7ApsMbMwl9NLVybttpsiVDg3/6QJ1Cu3b3CaUtmeD0VKN3QsmR4iiteOEGYYghSkJmdzZuQIep6rrVO3Erp/w8FnI4E1pfg+5hXEwbZNhWhwW6mOuDMMnJyCrRewe2gMeDKZLUm5OqcC8SkraldSANu7NF1mXDllR5mOR0FekAqRkbFF22Vbk3tVYG1gmNVuiz6yDHVNGdadDBJauS1WvfKv2BsLUensNMMGwIR2wT5mIDDQfrej0QuHbsZqDy3EKEqUqybVm9maAyap3HC4qGwzmQHbNY5t1QhknjXth3VxEbs/uyJA30wDwMAKw0TdIgo06eBmTtK9JhiFMkiOt11VJ6Pwa1iGtNNnJK7hqlABng9HRVGZjcCKKkyNdY5EzjdCAle/DJABWXpN4EcG3+g6igDmjZfAcO7UMXxL2mrA+ka8P4TVN6V+MDZFiGoRqGPsw7lg674tIYvyMdYHIEZECwq1n2wLRdl4ySO6VNvDMxMuiWaho7RXI1MXRBgoFafxsNsowTxoHi0aUhgUD8Ok7XPvDtTgZquAbE9JDB4v/qu/MBIkrADK7ZwNTpMId0aBEAjQChtPsW/ZbYq/AoIUtEsoNBu40GE3mgyrtx4IBzc0XCPJamG+YwJ5NSiGABuk99Frti3oZLJkb8XgER/zIuUiei1L9rbI20mZoGTnBgzo/tw1hC0BYsSLp02Bfww6iqomo4Te5N+PUyKQe6at94ALrwc7tp3XnwDhIoKKptaaYi9y6j76Nm5aIP4v8UHUyMCfOQHrK6BA1DXGEZHhbgknKYEqjDBm19pc9YLjPb90agbhi2bfSmXrYTqO/2DWsPEg0TJjKqOixu30SidKDPbEg1bgbU1GFiz8NhnywzJ0XjVK+TO3QOjVGm6qYx0GyhAcc9kE65qefexg7ot0x74Jxrw9iVS04PNn6WLEumNFDcbzFbJEnCpVMtcXFpRKdBfn8qALNpy1Z745etbNxbSQeLn6UplmoPXD3Q+KSak+yuH4L4WkfbUKxhYtY7rFle+Bj37Tusw5lmmLJqKsO+onWY5rgQIfpeWizce6MPs61yf8jaTirm8c5hzz6k4cZssHfDDDEJL5Gu4v6G0tZkTZMGZu6rjPlds0vDqbNNqNq/fLlcRuA7JAyZlN0bw3QJgmyYm/cajBVJHfm/P8pA4g3JMIeGtsR3/NjbjZrOJrFbqYEAEgS9j5pqLw6NcFH7wFTBPsyxbEuye9ertOWcB0IuiaJdI9nOvqTbKFN1DULFgR9YN21gWpG7zxesHQjVTUnCld37E5rRvEic6zi9nIbx9b2Jnq1DrKQMzOK3NjJuJHCFKqX46aW7Jd4ySt79yTbcC8LRv4micBELScqWPgqYiBKmIcnmYAMeCvmcCvXiWeGv5wLLRAh5LLhUCGPwRb4QRFiPn3cxB1U9EXIyg/fTOKUMwg9vXwspxgJpJogiQ5t5aZDkgpfGWRanwSyI8KQB3GYfF9lI8OmUpscjdhkBgpzO0iBfHo+yOQHmit9/+EmX5HNqJZffy/bpL+Gb+fn7SfExekZ/ef7SjvyLJ7ry9Dl9Yz37SCfn6eJ744X07I3x7Gl0de5f/fw6f3Gd/Sy/ef67nP2qv796cn32/HR2fDwScE8WkAIxSnkCxeQDuSKc3JGQpV59pgI/O6E6UCGD8WDrnyf8dtI4uGBKVF21fXX8IRudHE04tBN+1gE/KKR7dkqW493aGRPsZIqTtaNF2AkMYQxcbLQY8YNTunVRctl5Jn1I1k8wKe2eWE5omsetPHl/+ubpC+fd+7cXZ08vXr5947z/6dWZ88vZkxdv3/748PXp+x/PLt69On165rw7e/PDyzfPnZdvzi9OX706xdrnzRM7kL8iCHlwdTy64adFlOc7NBBOiWu5LnGnkqt6U9mmpm6YxFUtyyCK5+G9q06Jr2iGrrkWlXRF8y1FU32MaKZu4wSSDfi8LNmAU3V92TcMIrmeS2XJwo0Z06kiWwo1XTA5nqdgeC5PTX2qUs/SDOoSXGCr+pqnab04sw04bU/zAJ2pmhp0TNIkSdWmsuLKFvUNePBsYypR3Zq6iuHqpqyDa51K4GhMSoBY+3acHzajlF1CptOpr2qEQlemAN+giktx5ZFvaZJiWKoPsyUMMagugXCbMBqUTDVF9VWGkkvP2ikvIl8svFHmJu+WF3HqzV8Fs3mO8/hJssyxQAyrEgFqCx39G9YS1fTWY2F8tObICbHc6tOgUbdUy7B1+ZZjVzrNu8rYJXAQpOZi/SY5jfOSdoMSXce30LTOtF3hJ4ULZrOBojraZaeuZvgx4LKl62G2M5iI5ugbnTSO80OxrwlyH14+qIwqniUFLjE8Hnno9cDVhCNhntLpmncZJt38jJ0FyWCuM4FIOZvws6kmDm4nDbxJsADHnk1ALONxgsQMOmirPHNIRCDbThYiSTBu0IqfC65ITqvziias7brmVaBpmsbpvrB54+3AGy4YAigAfTd0NTiHg2sTUA8qDMSlGHhoVTcO6lrIkAQQtvsiCiBEHePsip2UFsYQA30jsf9xO16jICE7FyqnAsfDVhIej8CgszO0puSKl/N4hg3/hA38MILK9ll1w4SmQ8JwxNCf728W20S8HznyA3jcHGKIKxd4phvwqDGS38hUURUTKjfrslpi5mGTRmWmPaAIaGsmpfA3xzAKphSPCuNET6oCCOJYvzF0fVuGrmDzRS+lmNGCWVjWJbYKnKpVOFznQhZfiiGJZgVqZyd2A2omLJxmt27sLyte882YYlzkAo2uxHqDEgvfwWJlCXAvuKL8GW0DhK75MgRS8Bu3eJ2S5JGAya1LEQselzFlaZ384KpCxWwfN4NILoCEgUaqQLsQCsyChLIxNCQlr74BpU9zMZ6KZY9WYpLciIqQLEWNi7fozsQgmkJYEGEcQstSXJ8lXs9BngQ8N0wE/NPYKzJEnl0GiZjHK8gn51CAE5Ky4GhCVvRkCYm4pcOv7SLmOeKo4ZgqotJ4BkzLuD1ks5oUUTUeRZekwruynrDiyjS4gQnQdeDnc3EKk7MqNmeIS467xLvE0w4in4vrI+Eb03at6fQxa/dIkL59PBpAZYVdBLYshE00I5FsfgHYQQP4lbOi5MhqRrAqKR+O+JhWuF7wEY5DXygHuyFzwBkfZDsIMzYIJIig4Q+8RFgXmKmGA66MhDTm7MAMBbclDUFbQRJvQsEXw5k4DemNgH9YhwEXy+QJSUO+K3Y34PiNdh+KLA+mS9EFv01ptA5sJblMdksIi1TUbnHGIwH3xYPWumglXpQaXQ7gjIhspn48+s9XjGECMOy/BM7Oh8IsRkGtrMBDZrofoU9mWrhAS3TSGJcjsHvAfzRTMPlQVpIAbgJbCuVVxIYiJ3FNgUYCCtWT+OZ4JAmSIBvwD8p40A+x0BiCWiaHHAPr2zzwwZKV4ndylJB8LkyDMBTTAkcQMwWxDxGVfzx6bQnSU3WswwX+K28sj91qgjJWbMEY66qgjzVTMMe6PdbGElx1cSzziwot4MEWx5Isji2FXeWxZosK3I1VE666KvICA6rZGlSBq4KFFn+GZtATKFCgQAaoBlx1Dgtajw0VbwV5LFlIogyNoc3YVPAWyy0T/gG9KtQ1gEJszYDBCx3pgSJ5bEKBIkIVRGnzG3Vs69gDqKhiRegiUKMwKiRWf6waCEDBRyhXGKvGBuAA1IAFKUGIMhKmQn0FKMG/0AscMmgPHRYUAV/IyAyAxpiLHa+uY02Dl/AfjPDYRmQCIgPqx7qB9WSEyOrLOhsr1iXotTo2deyLLmBfYMjGig6jNzbZGMJQcCqQKYyHNmcm4MV+KIASRhdGWYexPLXglSrwv9BPCcgBcRC1sYaM0y2R/fcRzRTKFVqrq1lDDdF6N1WgpdpgFSBaBi8Bdg+EM/Iw/mk+iOB3ZjSvLDJMFhOwPCdtW9cEPwH4DYTbLMkWo9FxexP+hesv8IDLvY/P4fH7IvmOLJLHWBbG3jE3p99zc/o9WIfVWzQJx98qz75Vn65O1PxWPeMlqxM1eQm6dLigc2TtebRfAmdTllGHSKHuVhBBlMPCAu9SqJk61de9bxSLBebtsIHggo1CF8EdN3uAuilEPOlSYA4OHAR499LFywJYUhX/ZAtRX6eHGUx2rEFlM6vcKmYiHYwfy+RlmX0vIOaD+Mor9x7W29se8ktClugHy0YlsOoIFraTe7XKrgSJEWvp4FqQWhPvskUE7v1hTck6fecvn79xfnrXAsRTm0BANHNgztGq/0kmei1qWmeYsA79+edobRzE+YJ4MMEgVDEVqrguMaki+4pGdU8zKXVtw/SmEjV8a6pIJvVcQ6cWcXWZmrql60QzdMnzuwPe1RpBQPX4LnKz5HGRdFWqawbcIs/B4zVd7wWoUAgiCnOEWXkuLXtNbyDsAVFc5Q34TIRDGBJoNeKb0py4ecSOf25oyyIXZZ7o6oRSLaetaFuddg42g2LElt3FZyOCnX022m5TP4X/wz+00rKMN8oc3QVZFcsymn39hdytzJp/XEjCOgxzMAwTYbxmjWSl0UBiDbpgJEmswKz5DpZtPJrwAT7pmPaulW9aeB6WvaZRIdS3YjPU7QT6eZyIkpDi2MLVjQHhAm74aHN3UzUAOVlFwJ1wtjcs5SVgZeJ6arXumBqWuwmVRn5pqUVOYD3Tyigeg4fWOhHVZuz7ZWjXBnUaqkw3TR1adbOjRwrETNoh9UjHABL/kJYaSIYgvYIwT5YgNg0hoFJE/NNUClYJ/7ySVXarsHpCt57YqAgAsW4obqgp8Zoir4l4ASQjDcJv02pWlFilQUq0Fh2BNKxmSagB4OpV/AOCKAkLV9TxDz60J0nPwWeRsD21KcJWWMckGk99ENmsmcn1eqAVBnWjRvhSqxbTCMxLrB5betXWi/IJKq00GYIV9gfKVIH6M8rao/ZvmtgiihpZV623xY/CWr9Qvcvp85plqpRFqJQGlxSGZLl6htlwHfDWKYgO/KxY4CLuDfCrN40i5upgWCUM49SKTxJGg6y7LeY3o8mNuAXhl/lSeB7kLwr3LxvfMy1HJU1uRsKyvLZdoCZYI+FmET7KEgKB7ijBbqdXYFZQOY9HTF6q3jGT4M3pFVz8+BomyJf5rB5A4mZxWEDkt8lYb+kCEMlMAXOfD+XQGCsPUXkfyitF2ty19vSm9YIzfuPLprVP44R3Av3UyjNUATcfJ66NrqiBZYfOaBuktdnZFQdCOsWW7IK6o23pfj3FaXz7LZNnSwj3wzmmgsBwZuA4izBPyUpIXoEwiWK5iaA9oZjrwpNikTB5E0X2gyG75FGeVaScVHdl4q2krIadLRduHALzYpKLzH0LzFNEcbogYdN3rOYzSYou8ruUpOnjMpPWyCx2uFMbs44Jw0kVDIva+Ba/EUBt2FpGB2RrxfhF7PKfPtiF7bXHbzF+uoHxJ68Zgvtn4dEkDLYp2VBmVFI4KRft3CNbTjmGr4wv+GM3zFTeJ2uerpB8ZdwBmi/LLy73xZt3JYqvjDPQxQJXWt0jZ85LFF8ZZ1CjRFxVR68n96xTAkfzlTGIraLM7pM3LxmGr40tbPEiuW839bKB5lNxaHNIczQpwpMtr7YHSjBxlBozsjxOhITFpOr2iHzAMLAvhbhEYyjnW5y+v9j1vKLrhE+MVgWfV7qHsJRvF2OrguI02MGJfiLWPi3pEyr6Troln5HFXDk2la+ylZ1iPs1fS4VswNAYvJ0SHZvyEavRxp3MqxHu5ghaw8ZyBdtTAzuM4AXiPMG/G6Zcn6zn9cahT93/sxrzSX2/lRf/ypL9U2TJzm4SMDD0Xxmy+kWVITtEigyiCQVzMZ8gWXZrlHO3eIZyIfkCsm+luIKF4jdlhvdzxi+3RZ5zrV5pto6qSQq7X8RRjIE2DJiyLSx9hcfXCyTy2dJD/osjR5O59umlAsgKvHucXeziuzkpJ/z6uedqg+JZ/tu39zs/2yV+bdBz0nj48mcGOW7yY1sdvgA2XlTEnFR3XzwDqxVBIXHHrRWnXwA7mbFDJgJxpenDp1fks1r8ndiK56Hy1VLjWRH49/gtZQe+vgWiBE6VwKgCpV8v+3/kVZ/GUQQWT8DjJIU4n9M029epSndxqhuW5+E5BIv7/AR3MafCe0Dy+gz3GuCPin81ytVgE/9lrC9Cu844KSf8+hUyc1FE9/o1Zqe4pKQGf+azWKCmtgq+Gu5SvyjXEDf2u38JHC5nUGcVfavk8Krkq+ExbgDLtvKXTcM/L5PPkUC+owuTja3ShJd+fqe7qXyfPPGnSJfiJiSaJyHxPnm+9HUD9Unj4V8Z03/ujOk7fgjf15sx/UpSplqZMr3npYXlmYq1q3A/V26zFKzRyTv8PZFPsKrwk2cMS15/Uy5TEPkPqQRZY0f/Z4418XddBPZ7LizSrB+/mhio/oDYzOaUP5fzhbCZ0SKc40/3sKl3/fj5o58vYwnJ8FlDX1R7p/UPu0x66/nDFzFxuK81D224R5OI1JtV1vYNbztNoLmDhfWzfMHuoRT9V3vnykbIiyASeazENsKoq40w3b3HDx40AZQbdjNKUm/ecMPVaSrliy0xYkbDKShASvOq8cLnhQxIRQcU1hEoPKhC5sUJ9SvoHFOrCPB/QBnJ49GD7kkNa8RUpzDgUYL/MfpfPOKP3R9NkIlgt1E4bhbJCXvDTtGJUWbwJx9OjvBvY29Yo9sivqqOjeBF7X1F52yvJFMMRr7I96K9X+3ebb0M/NbRT/Wbqtd4KhC4xgGbcCtiGAyIonAIN0CK01m2Dq7VuIg2NVx1ln352Y0m/BlDMHveHGayND8e/XTxTIQ4d0HzeQwcmNFanI8YHyvmI7fZgR7AcIH97JCYLYSWhK6OfMFtkDBY3jyNFzRkh5KwBvV5IBvbVUK1QaD3PreDIS53IaK8Nfcd79AzDqakD1wADf2GFtQFbfmszqRpFrKaAZCKP2+O5/40KeKbreP8ki6Ps4eTxht+UtDva5Xxd8lF/iuieBwUGLkWyc36bNI7j0Mfz8485/LQBbcSuA2Vy9zIWpvtLZpV0eZ4JAlyEgYf8TS16bT5mqsxOE03duObVkO275JkSZwUCXfrm2p0d5523zP85UnGHMpalVIG0NKU4wpTPnDS2VrNysJsZmLVOivAB2fs66yI0zXQVPwx0WT+ezh5TvNz/pr6b1a7aH/A6lG5GrgBNktoGLKfvt7UvVbPWnytFeH/AAAA//8AtwBI/zxpbnB1dCB0eXBlPSJoaWRkZW4iIGRhdGEtY3NyZj0idHJ1ZSIgY2xhc3M9ImpzLWRhdGEtanVtcC10by1zdWdnZXN0aW9ucy1wYXRoLWNzcmYiIHZhbHVlPSJUaUp6NnI5U2lpNmFDOWpKSW51SVVNeE14c1pkMEx4L0VkUUxMMzk3MWpKUXhwNHNMNWFIbkdTMHY1TlZUUUkyOEk1VGVlcnNzTG1oZXlINlFKTnVjZz09IiAvPux9a3fctpLgd/0KbN9zMzPnhhTfD1nSHMeJ48zITs6149mdL1qQRKsZkc2+JLslOWe+7j/aP7S/ZKsAkAQfLXXLcmxn2sdik3gWgEKhqlAoHBH57zRdrtY1qe9W7Gy2SJOELWckzmhVnc1+q7QqrZlWMVrGCw3TaPOUZcmMLGkO6TFkRs6PiPLvtNpckds8W0IBi7penRwf39zc6De2XpRXx5ZhGMeQYkZu0qRenM0sa0YWLL1a1PBuzAgtU6oJOKD8cs1aaPJSMyEpTVjZQHTN7rQKYhez89MVrRdknmbZ2WxZLCFbVZfFNQD5l9APn4cvZqRY0Tit785mujMjydnsta27urswrdjUfWIQm5i6DU97Y9qxAR++xgM0+FtokEzj6TQbg+FpbzRMyEN5CCb9AKAcIyw9iFoYsFrT1APiXQTEdHVzoenhhWlgyMJUMmMnnR8dKV17mqSbpi++K27JqoDBSYulRqOqyNY1I8WGlfOsuJH9RxINe4L8ts5XWl1o1frqilWYpSIwtBPBWlwsa5ouWTnrD+rR6Tpr6m5KnS6hZvkqo4A0/aKOjk6ztCthnrFbgg8oAzLOYRhrWtYiCFAuB1DYsmYlWWkGmbuAb5v0ivLmYjTWPhE0BmhGyiIDJChW/AtBOaWkplG6TNjt2UwzW/xaFtoaAssMgCYqiHRdFxOQTbWeD/kkIG2UAnaxgkFaaTgDSjY/mwF60JryWjQxIZWGiAFRkaCppIjrFHpbrbcJ4lBXizJdXkM/wgyySM1u66YJYijbseZTt5mMptfOUXyVdTYFy1+tZKtiUMsYDJFKVkbqtMYR+TvjCFyUd3LSZzRiWT98k7IbQPWzmQHT0/QIAgI4XkF/AFS62Yxuml+pFEAr1xjKNmxZJImYdBaxdPc5/OEvlGaYDv4uAt13KfzBfx4qXjcmpFKDNfG+0HrhpomzH0gIZtGshRZQk5iYwYAMpoM0RC3E1A3fgkADIXFChAWfGAfBUNJGCz/kQA1czXwfQnGxptuuBzRH90IHMjtAAnXLCN4jGLIqE8pdBB9euwSAttyNDQ8Kf/CfQ6I7upWZuuNi9YGvRCF1My4C3UNK5PfzaLq10YZFQaCLfwuIGUXA/xH9ejRercriNxbXD6NWk3CAXb+I4D5qtYFPiFcmH9zn/MeUA23wxUMg0QsDO9cyPd0PHF4XTwnEHvGxnw1jYbDc9xgyjHN4X7/CgA9Qq4jqDaYcG4G6vXBEi2GwHLJXw2KaMX4va+IQ2MpcMORc2Pi9KWIgTMZGG4RiYvz7kGsB1tKfbSKPO5hrsiQI/vA6mK7bnqwbQp8roZD36TBSsBwPI6RMN8DHtzy0j45N2FNiI3aETx0gKyERT6RS0PVhGACa9iOg/3l6GBs9sCDYd6hHPJ4FaIWHBMPLbN1wCD5UusejCI/XMI4/Jjv79BiWrHPByJxCE5r+pRtY60qxJm3tVGX9lOmbfqVZjQum2p/vGM2B7StjjGjYy0BhL4NZA8fEKqpUhcztFih4VMcVDDkuvrhmbF4T4CZuSroicQU8UblexsAT9T40aM0Vq2cTvaRAFxUlsCSAAMiaJED9+10VF1lRatEVsF1lndLyjqxuIZEI5sC0EXmGuT0yZt8imlw1TL7CC6zoUhEEphKLGhI2p+uso8DqkKTQJYu0ImW3qHeM5U+jyKbuY6x8f0iusiKi2TZAaJaRYk5+TOtX60gB4zmEi8B7qt8uliRaukSWUYuyIr4W3bzRaJZeLbUcMmTAX/2///N/1UJHg7299E82/onGsXmjzYt4XY3RAb/aTvo3+CB1MTUgj28+vtDzo9PjLIW+OD1eZ/hzn5wBc6pkFeDaVjHjASFDsr1jXh4kjPvEGs6kS+DVxiv9XDGAJKGI4G8KIrOyhEsJ0HVEAt71g2i30uw06YiRTN1Q/iyt6qi4bbEChaJcUEpFnGi7YlI0aXpOSTUtwY0Sfg4BLgbZKBmupV+3KMebdNkjswdp7iDNHaS5gzR3kOYO0txBmjtIcwdp7iDN/SmkuT9aXihulrgr+OeTGnjDLg+yw0F2OMgOB9nhIDscZIeD7HCQHb5i2aEor+gy/UAVmxZVelCjD/LDQX74g+QHgSt/KslBNOkgMxxkhoPMcJAZDjLDQWY4yAwHmeGrlBkO+w0HeeF+eaGxoyLtvzaZ+OADicnnRZmft9FqX/bytSHqdGpP3JQsAzZ8w5D+2CSPNAcf2RX0ab9FvWM0ID0IZv04K67S5b+WrF6Xy8u6OMNzUtVf7ed/tV7C/6u0XqwjPS5y+Pjl7l0BKHqBRGiZLq8gaHVXY5CWKWFQWwQ/Oa1gyOAlKeIKfqpiXcYMXi5BXIIVBt7SnAI5ghcAotBXy6tZ76SPbOorfs7qNVuuoZrl9QBxVHmnn51LIYu7pCy0OEvj67PZ79/8Y13Uz3ChqS9RNBHfJ+IH6OACItKYizU6zyNivhU/K3qXFTSRmWRh0LU8/WW6vFxBc3pF4pE1eU6M5NCAXnEdtbhMm0KX6yz7tgNnDOTbn358c/nrL72CijKFMQQolleX6zLrpefDeXJ83A3k8XAYj0eDeIxDeCwG8BiH71gM3rEcumMxcMfNsPWgWVes7Dfov/5rNhoMbZFTWN5iO5g7lh/jw6ShwVhoR948YtSDp20kjh/GbhRSP/JZFPiJFSRJEoa+b4deHHgTI35Fm+H+54sClruEFOv6X4jAom8Jj4PAtzCvSbr8lq90JxXO8nTZmyOkSdObkrRLMp6x6sTCItcrmFnzy7imZ1jW39arb2i+eoZhgDdnAjP+lnEw/wZgtrGISWc4Be0X2J98tf6r/YMIQcRRQuR8qxbFDc8vxkoWziVWJfgSv892mcn9rt1xNg6IaE8ZIVcASen5R0fU5bqAVN8iqzvNHFZ/mM3/3WbzI8Z8SbM7aFGl8wQPDzeyr1cwZP1BwVkPM1XNTWPEil4yDhGaU1ciPYHlnNA4BkTu18zX+17WZoafHH+TiS7m0/mbq/oZD2knOA/hw4ez+5mkJScSwoaOnEiczFpy92dBKMOzQsOxacyCwGK+F8Smb/mWa9AotON4bpmWZ88Tg8WGERuxM3cixigNjMBkhhskfYQ6H6CX7MY+/e4IvMK2KYye5NVElzesb7sQcBYNree5Slgr5tx0HXCx5YNxIJFj5dwqipkiK//Hy8ISeHesfqO3SL4zaL6wyK+0OZ4w75nD81qbIwiKcCFSNhETSleegIhkcxgGQn7/XcS9QdEQRuX8qJ+DE+fmjP1ptK7rYtkrDcauqLhEpnzO5IF+kb4vzHyfVnlaVUJ6yllVAd70lbgTAoWq7XhY46DoQ3ivYg7owHwFssOyHggpQ1XJbdNv9+oqbN23CD76CiHDQ50k8fTQyWzdslAJaamqh1bzcBHiW8BTkWGqTkEBhYVSTzFKJ3SyQtFxgVWSgIOEupBATWbwJJ2C46in4RCDJL6aYUAEBdSQwwOIoaB7T3TB7wbhOnlGlHXE3TvE2Tph2rykV7B+1gq2Los6ncu1GrgJls21YWo5fhGtQFZG1cgE7VKLqY4jVtNjXhi2dljeeTPnsIFHCodDV6usASWHaUYEFeG1QyV5WmsLVJfEtEwqjS1plLGkS5KkFcxsxMB7kwHar5lGl4m2KqcTKowmpyq4a4PGV/xNOskQTi2qeMFyyp1avC3m9Q0t2VtOkV8USYfcDS7zNkl6wlcaTmg6mqJQn+6ATgPMUYcaLdE66lG+jhHqsmuCXLagLG75+ieDQewnqxqkV5jnjNMirYpLxpYoybqzMTVseW6VOsm9LshiI6HijzwBcRh+QRx2Z1sk6U4HlqdLjZMLYGTF7zytuVw9kAtOF2ZLE9UtNq4nG2+xRSWj19oNcLxkbgvN2hJEf5q157g2V/duS02d7eL6xj0JoEo5x4T1sO/0Re479ZRUKEYU8tRghVSyqktWo+4dMW5VFqsmTYNbtMkKrB+ZoxMSXHhlGsl3NdSnMfRU7SKGSaAYEG4hSTXmFNtUzVbwKMnsfBgiGS7lMKLa3vy21RQqrZ2cELPzY7WQuiyWV0qvIDut+u+xJjqx7bKW4pzN/rKNSm5p4Zhdnp2PgrpGczD5oUbgKU3Vy86EPrA7kIrUE4mnJgSTaiSGS9Vo3gjk6IPoDuf0iiYJQHBCrNUtMZ4JqniKmktZSYswdVFkdbpagUjRvWoVieol/mlV3mfm/lexJvm6qknEuEwE+VBrDuRrQZfAOKiLM6lYjQJJJfFxWcyLLCtuxpLAZ5Dz+4CuI1iMUm54waV+ckMBVz5e9r/4+cfLn978WSS1JGKm69LEZyCPRW5sRC5zHdOMHJb4dhxYoZ24pmMz2zbmRpTEfuJQ35z7rula1LNn2/TRO+qd1aX088oMMBeyntiAi2cAtVBYQeUebxjgWuK7sW6YvqabNjyM0Jerg90tETFGBoQvD5ZumjaPho/nTWFY8pSjr6klHNLCEg7FElss4QYu4e7GAnnBjw2i244HFRqW7gVYE6yYoZ8hpIaNq77rUQA4wD+5aBmGDV+Zjo0yDA/LMCC7+HPkH8/l459YGiEEYzCXjzkXsF6HnvjGIk1M7+Ffm14TRfVDHQz1eOX45ver4cBpCJ1oADIerkN7xggmb6QGrXzf7xfeUx9e28Sl0EGCSYBH11OGCyXDXwi94cDI2GHWdROaPLgmL0P0FAyc7gTAodivLAyjw3gA0fIQRNvLFGifKxyCTXys/L07Lbu9UUWfo3aBkSZsCoHvE/mOmPOfG6AuGldhEbKF/u9I9FEBQqi6X/oFUnoOpJB5HyToph94ludbxn8jqh7QZM7mse8FoUGNyI6MhMU+i6kFf3E8h0A38TzbNqPIcOM4dEM7tqO55SfMNpzoiaj6F0HXOa40G8Kc9YwKQJwcBTFzJz1RQLg0oIhKng90wQxgygcBanQC0yUgbECE2dPtQCJUAVl+KIxUPCAzIQhGJprAmOFIDxSAUBaa6NYRKIrjYybfwxLCYJwUBK0w031Lw6KcCwAClVa27wwhQCIVZAgfSFOmeeHrtuXrnhcopkomb+SH3EDJzHEvPEgJFHQoC7qeAyXyxkA7HJTjLAceZtCvFc3NPFgaIIeFCi0jwFS+F2ooj3n9zkTDoAxi0QUmREIeW+NZhkVqvEysUsMqeaoQQp0xlBCI/Wih+LjBlWaa+jZiy3a06sjvW0CkIymqtOrmjiJXRZzSTBJhdHuqfO8hdRwjvl7RDzABGi24SrdNWHl8gsSg4qS6RCq/xcAFzf6upYKQSr3f1Lqyy6rycYvKvCivv/hFRdgIM5ATs0pAfFhhtm8ZmizyaOgZrh0mUZJQ12KemcSmZTAW+JZlWnHihG7CGCQxYhAlqGtTy/PMODLo/MlWmC9kleGqBkQaluy0piDLarmD/QJ8GdqmCpJsWhZFHRexBFsp1G/A2OqBHzxXYwyD5w+4cg5FBNMKeFaTiCdm5nnfyzS0nx1fNK5P07Dw7dVOagmdngEsLCFeW9KH3MZgWH3UvPc03Nb6OkrT4NUr9rUGZ/e3UHb89xJG5EjdG6R7UOIlq28g/3HO8gjJ8RSlV3Yqe2QaBGdJpQVS3GOFCEzEdbt1qbD/PXu0vlZJUdpXgJ5VumFazvg+x8RuQKt9723t4L+j0yXd7KQ123ZA4uGZ0pSINXFv4DBB6UbZY8CqCE/VtqU1PsFMQxPXX5vINxC5fccAtW4PQ6cWBoxhckfQUZrGlW6DsylZukN5rSENKvaaAz84UnGRoMVttI8CUtQHmfi+xNksNbpSeFTFMhbXsBCjSU8l9nAuBcnnuHaZFDdLXAYF6l2KbTD5ASswoxWTXzW9km9RSZfxoglf0fga1wxZHqBScYfbcBXZvQWLor5md2ezKxLPhsZeHTJ9S960h5OEude3BDfCSNdeiaQcn+QG/TSmxmvgjHBwcJXfBUt7eMCPdi1uL1U0FIGNUfV2dG3PhbVjMzwz1kTMetu16r8vYD1DROvNtPZEV6ujrvJGR73LgueIzXZrvNnOt76tZid9tCXf20nXHR+XNnu8M47yjas5XIjxdNw6UhcOpQjT5rvsXLYa1WV0pwra8tRjHrIuravsnsVHES0ksp7N+M7uOT77Rt5qDnli401RE7oBbhQ3lncZwxe4IKFxSVO0wC9cVMSK8iSEjG+AV3uSsmORa0zRTLW47TRNpBI0iK9C8j1PM1YBj852Ikd9KDqqlO5JlX4S0OxHl74kGvRFkx5hYYGnU4FtegwJEjqbEDeK+Sm1hne0SfeNvOPkRRvTCiCDBiQQxUBzn4sPVDcb4ihcQIHmEE+XBkI2/HQBqLCBkH1JhcCy2bn4fZBcWIGxF5GA9J+UTqzQHqRk/wDY673JBWaeoBbWRKHbiQYvhAArE1/vRh7UWjvqsNqTOvyCRnkNjAci8WmIxFUKnacgw5PyKr5u+j5qLX37AsiIb+ue7yt7OrinpAeuswGmBpW46tlVx+K5L5oy4FtNYKBdi/PhNReF+yY1QpodMBt4bNWSx1Z7krhpC7XABov2hhK+FKrfuyBsW339AJeuUfmAR2sRhIVmvncWZmdVQ0wo1NuqNugZExmijA855DIGGo2RXC+Ako03rYfbvifF7M282Xnv80H6CTDsRT8h/Seln5094t7UU8k6pqH2qODtFFRJuhMBHVfckdGrPcno90rdByL6iYS8HEV5xfT1ScmoycnEtI+AYNJFgDvpIUAa3aln8F2UJC9w1x+ojOO8DwdyWnPoH2jblGsBebRf2BSMXCcgzAjKCwOYyNZxgiEdJxivrA20zLEp7laJPStfak6dAL8NG6i/4SFFXNi6GYRDTwoWCRoghlFIQ4m50AIOH5p5OP0eVEwRpwwydTfWQ4+foOdxgUPaGjaj+kzh1MG0XpnOdKNMTWlViPbraGsZ9Ml8c1Dfwnh8bHB/LhxYl/LxMHvGlupo7EnrFfIwO1c+vlaxWhoE7kvqZbYxmXd6BW4n8Y0h4v6VdaSd7knan8s6D2T9k5D1VUbvnpyQ7yThPiApo3VCSGD6Wyo3bXgQ5DjIyZpm3xgbkgMBdlAX5znctCpU4w1kt4MLpdh9qYjExNm5fPlaqYd0QrS/nC3zjemH2y/yHilbpiNLdnOphpDexyOg6QhMtK8I3sDUwX4gHDsQDjlWT0s7Du6pvgr3VDuK2M0UPW/eHiSZzl5k0vmkdLJi8bpM67t96WSTb0wnvX6Rk3SyScK3shFwQjNWAnlaFUCv7khdXLPlZRXTJVZGcO+t+3oEeB3hrPYknG8bSA+Ecy8b0wVeaf7EOkjH83XTtgeW7yhogXiXcXpk6l4wlOhQuecE731+/bjreZpu4zanGVh457hhg0DpwbseBrgf4oQB8lUBSuqBaQNNMXjywMXdE5T7/Ibm2MDhxSgOOnipueGg4anhhGj87voio8YzvjCxShBlTTSdx/1atGEl/nsEbNQcy8TiPM/nTcLX4EOuo/QMzF3Yl3xNF2DItLbtg1gf1gC7abrtAj3lSlaPm9AaLqpjA7nOoMksV6Ci+tJ2oFNsSGXrVogcqC7OaEB9DrQZDU6BRocuJnCh7yCfIbPgScEweGGiPsIloagL9RIBxIo32e4hqHgixBdtEY1+HWKfNacbTZC88UcqZS00iOJLT3+JaC2y7Clyvy91b6b/7Lx5m6TuowPh/Fz3PoTqGCkhzVrzWBrHbIWzjN3Wx02xf1vUebblCPinWBxSIEsA6GM2qyo2Xhn8fnnbOegrII2L1kJnWZdptIY6KpKwFQOSsozvRBoZQKOivlyvoEAGSwhWTlasWGWMoEJvvUT6vS/cO299yzYdFoj9tqv48D21TG72ucWGFCAni2edHOCJbdsTCbjyzdFH1izI4EpeNkceGm3mXXtQauOgwjT4YSZ4ACEGod0fpSPGBfdyEfTdanR2NRc+8YWu0PHxrIE/SiXWNq1/rHrHLXOJnbPz5u0zCPTCbpNXhGaa8vzuJq3SKM1gbp4ILHy29/RpVHX3zZLGWJK0jgZpVBXZugYagY4G4SFMJUtuKklK7CXNmJ3Lrmns33cgoyIlrxEVTs03QMZqMvJzKAhttc5z4TYsU1zL3HP6Qhk1xffDmKrkpWZIT2XYmsF4f/b5f80iWBgWRZl+QIXm1EHT8EFrFT7nH06Woypud9OX8cwanOWvSq1YZsATvC7KCau11peMHNvW+9COoyrRhh/RFmiBr7t0clIWKzRwFXl7X1p1I64c7rWqmZe9wIZn4NnGdrYCZcb5ONfQwctXIMXk+UHrzw5ekXWr2eqXb+O7h0eFiW4k3MiX089RDx/3j7/eM2I9O8KnHrPBUP1hZop7m1dO968wIPvYHp6yvfpsHf1Y0659LdCmO7RnWPKx/Tq2x/hsvfpR5h6PM0+Z7mBlP/dju7e/D/rZuvZR26z7bwNPd6fc3PrYrhxsCX2+yf8pdpwesWe2hTjI6I/t7YFi+XP09h+ot36EKn66+xvF1cd2/0B789mQ/TMrh/bVgm3hQGRf7jgo8uQjSBaKcHDeOa0UoV0AdK7i5BVd+SuyYgwccDlPb5FaqE4e6zRn3L91dxLxNtt2nnDgJ3GrjYqsc+BLEVWk6H9VaAoUV4z8B/+fSkecjTNZ2ikrm8syVgxdD6KfsArkyDpe14Pdnr1mET8ojgezKZsb1Lfhz7M9w7X8yLdcz6M+84M4dJz5fG7tcpR8dv5LA2DjbQDa8T80jWBVRKIuAUhPeEgPmU82lnVi2y6NwtgNWGDGoRVZYeiY9nzObCOEqNgwmA8AuokfRSw2o8QzI9uJLMtyIsdgNtG0iftLVD+Pwrujcutb63hthd4nB74gc+z77rO5NC5i9Q36uGzS9C6Ta/H/6N5LHyRGSV3LQKEiFChD9QpXcOQRPGYcDYUEJ2kGnyCtKy2pY1F8OqjXvkxdtiAx6KYf19wcdJOi775WZCxKgoLkl+TjBU8DCPh2052aT2V9/97redY0DdzteuV1zjDRmN6cOoPfs9AfFC7VuHpge9xdpkXEE6vwiL9w2vKhlI2GtvoDG/+wtfGXBp4Pt/S1rY+dEmwz4b/XmUpDeCfuGlLWWemyWDiruOfCnXaljGmJtxX1L3JplU5T8+4tnxyvm7kxHaXlRdIp5KRL9nEq6Ql3EkQlGZ8zs/PBnDnGCTNoY98Pt1IE9779nVSN9n1wiy2m4uoqY+hsYpoKdPNRntd/wb17C61ab57OKV/ju8u9Dj66P4WP7r577qEP+nS5WuMdVvn6VvSikOWg92ikeEkQzjhPlNR/qWDpZD9hAJlKm/QSC5aQp+6QuJdtasWMi2yd84s9G1TWEJWRJ65a16Tof1kgzonwTNrycdMTDsa6VqZS1wvNFiqSCtyObaHXBe8xvGcC/gnXCktxpyjXfKboF5/XAD8sSyYyjUES1ePFTrw/YEZMZONzBygRZ/DmGncU0XTLRPJ2awpKw1V9LuWLZLfs6NqaXzYwHYXTLGM1uv2dz7cBKwnAS94dfWo0kYV7CVkUGSDnznkEfcLuVyMVFLjnLiyVbHJ8EoQI3rFvZtuxoRmi3s0Q2wkqF+D6hLStSVLERvRqNii+k83uT92Pquj8HV8DhuVNdlCbCxYMls3INM6IDpK4/vDg3TO/J+4TxK+ZclVxf7oiMKLPirLPM4rWC15szGmK0awGkxvF3YnEDSEcYJ3Iyl20A41TIfkLj/pBxAxyIalrRrmfCWLeyohBHj79eKf3c/Dwl2lZ1RcQ95q7B+pl7YP8jzUDQRBk81WR4rK4i3wG9Q26pPcRUxhXjTPtG+PE9GzDMwwQ4PTQ8izHH9zHJnaVJH1EAgkj9B8vPyQ/vr8bdLu4SVGi2LZU6JJb+IYDYojo9uur7NfI/rff6A+3q/+0FkaUZ+v/vAtv2P/8+ybO3xQX/6GE35wNRnQFbUU33ni7Sl6sK4Y42CMmfeVAe2lKjz6oA6Tz8l6C/MSSd/2bVJRytnCD8jYTUbpE/t9/J/yN367yolhnCXrFJrjr1k6y3m2BsgrlWo1h7Tu0Ylm8RpfV77bfBTMB9/mbol6g/g19uS6Km1ZJooDSBxJl9Hc/f//zibzOhd7K1ZykqGKNsWAQJ0HsY6hNqhdMtpm7SiI3xfKfalLFQLMynZCbxd2/ckl81NfbG4rlvOiUKOou8LihvNKO92ihPSG2baxudz9QtCNC4CBjf/KbLlYggPedYWEa7CiZbEIBxvlxCW5U3GpV+oE7cm/AgrBnwjP+CdnQ8p81TbjJlyb5Kco3//Ks5bZtq2PT7S03SSA3jd4VlztZtNBlmmtlUTc4dhqnZYwKQyg0gFR3/AfmuY/dXhbX0BBJU14gpE2oVqxonNaQHE/2taHNRbgoPeBwa2w+hxcOn1bFqDK60kTaGTnmALRmESYePEJn0xYRTy5U+9pDkHR1ygDU9MV0BYiHF9btCsqEaQQhkxN9fKPgbnMcVVK9Cd56qPsIB5BArtZl9sMSFfLJ39lcXgy1kxpvijnmKuq+krsEbC9GXjyFEQPuyiIrhVSzeiGGh7S+JKdI6lE7Tz7zgWgEnahNx9jBtyaS7aZpAjkxQNfAPXHSkPdW+6ib8QfOhRszOn6YNuydtTVMRQ5FY2GQfj30AOxNCshbLotSNRec8TPvuSO6tSfjgyn4SsCzF5idDx2Ely2G9XQzvJ5mKHl2WJq+l3c1IzpIGC6QkBL+1JR7ctRrPrhmVTInymUh4vbebQsbJJgXRT2pzRER0D/7+INsGe/z92IvLFMWf7T+E6X2jaPU2X6+/ZaQrWw/F7cVi6hpdn+ZEIpa2Y9l8if2uR7i9bHWPy/vvpfU8jSM/p+Ht//COXehlN2da388z/4EsE+z9TvBfeBEDpzIgRP5jJzIU4u/B+n3IP1+edLvIN3T8t7C4qDlu8XSvRvPPQkmTFZ176vZmlMYPMWiqHdxrjQ+OjoauGRfWMIuA5Y9DUewRTC8RjSJy3UeKSx4R1PkZaPqJabKVaMkv9Ws5lZTNDsWr3miFCNOvZitRYr45nZDOU5beECoeiqmR3kbB+0l9CSRF+RkCe4j9xNhowBkceKzPUQJ7YWF6ibSxG2pFC9RG9zEeK/F1B6HBzhAU5cytleytK/ip3eKha1oSWu8YPN4Kv7zN/C4LhlTL+9oWozvE438ylsn2b+mkeLrT9vMhsttmis///TtlVx902zx9ZhWi3thG2YxXdJMUNnzRl5QL2UVxHxhte/7LDVQeCJbNJC1u07n3UVX9bpk0nYAzez49bTqHa58GCYUCph/uxlerVDqHwt+fxLIw22zaLfmNBaE7cKTFyUyIuLEg4zd6SjdH3GYU7FInDicN7RawtOGRDblc9kofQXHNrEfH334sruYeIezlEq+LO30jILUJMJcdjmh8aCjOoQF/MgwN6IVE3cei+mw+21gnb31rjeBCUGsV8jLn958f/nyp4sfLr/79d27n9/sfe/XV3ydV+gyw3MCK0wsw3BogncC+9F87jOT+gnzzSjwDRZbzHDD2AlcO4r9eB4mie37LDCayTXtRQIJKidh35LmArYTftlac6xkNqR/6kIlVS+PIt9T4uhQ58B14OcdnZ3yGDDKKa4wz1EUmbzTfEJ7dP6uKXl4CGLiAERzIqVn67mVwI+mnLRQGhXx2zpf1QUXB5vMWpJSwJ9JS9PdTl/z2YzXBaLtf0PY7+++e+b+5BGbewcNs20btCcYtoutCNH5f0AKLA27Hh7YYf8BuU6TTvEjiOhk1tE4w2RbRQUtExil1d0+KNM3EMbMQmbd0GwNQOykn90XR+J1WUGf882RdnKOurUFBju138CHO/fT9lBzwKXtpo+g6k969uYTDcWnmLP9nnxwbu2FAuLc2OicmIhqzoaNjKiAfyTbtkvbA0J5pNmd3fTjHWCpp572WNnbYRT7fFwt21vfBg2SxxXQ2ZLMUaMfiAmmS6if+tadqir4mmUM1xK6ofxmcP6jIXMx7KFMW5ooAuGz5s8Inq2umjPiJ5azun0mVdb8fXY+Uh9O1t+8aJxmI/nWXKzTmp1/s4yq1bNBMfcZHmMH8bv+tnYHeQw85hZQhudiupUnQTt9GCWPiAHVWFmi2PuCLnGXsmSAK8DyEhVrCK2FoR4eYBzOnxb3Ry7TBpNATIRuLgx2J2iSM6CTsS4VBDN1snCl4kgm5GV1599EufxrjJarOxCUV0Ja7kagRaVGkVkWNxNj09AqscuiDg7v07xYAiPnKbpW7vhI0ZRaA02p+TQTO2F8y3yvab3duZ1l6D759++OVEyeONcIPWniAxW8Q/XytF5Ytr7KtSvo3ybX8HQj8EAMr4Hmv3kymzhZ+V29/LEs1quu9/bRspT0Zi/6x03g6Y0GgtXsMRoHeac1aaDufMbB/++lCx3JyjV3eysk5HQ4o9U7s1tfbtO3ZC9vhMeuvODeujJWVWi/U+Nxk4GBjJB7ZFyFRHMJi3b1LQhpg5T9rfmEVdd1sdI7RmSQXGVmfl7BAHMSgvIOXtH9Y1q/Wkfke1HKFEzTkh1e0kbQWoRIAMaiyOffSk/YJo1R3lEh3MFpNV6NtMVXtL+fq2jp9Hnkiprf8IHHM19NOLSW95D4wkW22bnItqSLbGuBh9D8WNPRV6rh4MX3GBf46OnV4qdGfddXHT470re21/cDqLse+iC1HBsK8yG7HgQYYIY2PH3TE+VpvDxR64RnbqNp6aRnbvPD61A3zAAvG/H0MLCoi0fo+EPeIO2JPrfRX+xULDqqvdcjoKIa5d9o2f5Ps/+N5uj8HXfob2GRY6h3vs1X5zzmWHD+fC+xzM9P8dngUuMpFGcqaWzadvJxg9zBDopxhcIJH6h4eK+scBX+9d1LLZiRnNWLIuGHyOvZ+f8HAAD//wCjAFz/PGlucHV0IHR5cGU9ImhpZGRlbiIgZGF0YS1jc3JmPSJ0cnVlIiBuYW1lPSJhdXRoZW50aWNpdHlfdG9rZW4iIHZhbHVlPSI0SjZvZUg0T3ZpZ3Nqb0poT2pDdENZRVNsREEvd3lOSktoSUplcnA1MU1XZGFTTEdzamlweUd5enlQRFpmMGlqTmNTYXByWXRmbXFrR1hQcEpTUVBOZz09IiAvPuwby5LctvGur4B5cC4ChwDf0u6mbLlsOSVXVI6jqpwUEASH9PIVkrO7o5Sr8hv5vXxJugFyhuTMSivbsb3lrEZDsNEAGo1GP4CeJ2T2d5HshqGpiSxF319ayVDTRg6FBNCsTFNRb1VHhqYph6JtVTor0vrWIsO+VZdWv0uqYrCekMWf6ApBS5Go8tL6W7Mj1a4fSKJIX2xr6KmooTNSiWtFmo60XdM2vSIyxyF7i6RiEDQtepGUit4WQ3616v6iv9maMfIiTVV9aQ3dTlkkV8U2Hy4tFljkplC3nzd3l5ZDHMIComGq64sG0JnNLHJbpENukPWI2ILKpmqbWtXD1OfIpolFE3uGTvS5ZQi7aMWQk6woS9rtSmCKulF1k6bQ76X1TWD7hNmhL2zuwweocRg8KfzPub0Gw+eNm1P3DTZ5V3nQ2AEAB7gACHw0JkzJ9l9y3e0MShHq67YvfGKHkUd8/e2QQCPhiC+YY3PgCPTBDA489XDfeLYXB4AazPvV3Xoxt5lf2oGu/gzRdRsc1/Ftz4FXP/dtFks7xkZ24HNqB5Hu2mNIWRhgewrtZ0SPnVObeXFJx/7nTAFGebHNgVsUuz+polinW2LP76yriw2ux9WTiw2IyVJ0LjZG+LEya7rqygDT4gbxxoJZUyhOa5/SrFR3BGRN0XJrnnclQSBtulR1lJuXbdfcUseaxrxI1SCKsj/0A4KeNrc1GeG0U70aDm8NiGcp9iSlRV0WtaJJ2chr6ziDi35XVaLbn9m61rgdRN827a6dhHe+D2GP9QXsKCKgSVP31pw1v4Edda0SkdC86Yp3TT2IcrG5cCNFJBZMbyYjlQ51yfHdcRzivvsGXz6MVjGXOB/EuleU4NWsxCgsGrYr1+tMK1XvyOKN9rfWSiTLYq3eAChO+ioGVd2rjcn3KEwVSBDd1aXqe9qWYkAJX2tm/acXZ8KAQW6LGsbon1ZCnsXPO5VdWvkwtP2zzSZV/fXQtPYWNPMusWF97x9kK6gsC3l9aX2rQACLoen2T0nTqpqgYidjX9YpCwj58wHrC4N1yqaNWO/vJTdPmHsPY61xipvX+++aTuavUOrrot5u2v2AAFoeIJ243VSiH1S3SRvZb/pm10m1edsPAgR5U1QCjNimbLaN3dbbk5l9AWOWjUhXdC8mYqYxE7ddedApm1FdjLvDqKxJc41IT0b9hVND6wo7T90Nh50Hm5gmTbonLXUIKJmE3nYCJKnTa6YNO8UGhGyLfgBUreNQrx0U21E5IiKVsL2heUvdlVZpxdHRMN3gULCximpL+k4+jOVI40fw/I+wRJejAixB7RzXAjYuUDTj5RnFf2TlkydndPj7VffyHZyo7toiBWiv73dVOzRU6/UJKy0EkDbpuUm5632TN8O12gPpSx3+J+gFXSfsRs9mUkNmVRb9zlab4IrP7Is2Z9piyabcVTURdVHRTIBdA9csA0brEZDm940/sfETSskfrL8TSq90+WKDQiE6JYDEu6q90jWbpkW7czVa3gv8nmgE9YWD0YlDuu4gpoZgazRclxaWpGpB7HLRwTJcWn/97ksaWaRSQ94As7dqmJvNom53wzQUdg3TroeuGU242KEz2oEBmCZNTYsTqgpVppPja3YUKFGp8qYEyV5y59NclWXRPn8P/wgOnDVy189oHZ1zLQSyBLd4XE2c9cLjfoCFBf9AswE+XzVanu/zfxaCM+6ISc08OWqXJycKx7SvRFEv99G8ftRJc6WRNQ3qC1wGaApaAdwp7TzQbFeWoEdge8GU+uIGZgMLBdNGXJhiUWeNntS8O21ZQDSgGRg1aLSQcXTKOoWeijKAcoswU0Z1dIDWzfH1ewhZimw/qbYJZQInarhVYJ9ACgPSJuABVljKAphU2XRaf9JeAdUpbupR+YERG+tHwBFjUgNHN6JE5dsPewiCaljexZw0ndATZVw/gDL/o+iuNMkJVs0c1rKYBtf7Ab5M/aeyaffPCXc4I18Vw8td8pR8XUt7aW/vbQ5Wd+VCgBKf+Q8bVW/M2wYWUtG2AZ9hP4KAleCk0CYDZnU3hVSj6Od7MOGTd/HPT/+xa4bnGHgNb3GfmPdn5iFqUe7BUPS2RjDAp+bRij2a4xF/7EeKQW3BVVn08qUW2kVjo5EWWNsGN7imeYGqNcACE0Xk2SkiOMDbogYxrrdvd92yycS/Get+fvO5oGYHTH9bTOypYXf+8MPpAtAcXMdLK/QjzmUq4ygNoihjkcsCKZI0UwkLM8/nQSYkV0GcpRHLgpDFWcKTTCrlR1kiQVK+Q3agN/SLSFbbFTdC7inyQYF7PjwG0RqJfohwnUN9xOIViNDnTsKD0PdCJ/M9l7uRBJDj+FEYuUEYKD9MHBlmqfKlnzlSyihTnnTTJOORdfXaMOTjBOy3LxJgRXZdMTxIJs7iPmKhiL2AByplbiACyZhyhFAyTOMo84SnpJNGcRLI2HPdUGR+IJM49mKe+ODseiFLMmulQGbTnFhlXf1lLP0UzXR7ezsqJmTBrtdDPAaFY8h9kGydYj5iyfJQw4jYd0XkOmnoZl6iwtRXKWci5FKmgZ8wPwYr54aOx2QQpKmKvQjEzedcZCA2mh9nheagWI4nJGZNnhLDdYgf2qdEs/ULmOVaTFd2zrpCpOVI5tjAFMUiDHnZVKoFLkFIUQzoXBufzlq654drgXTyP0Gy9ZkkqbRP6d2/d44u5QcOFrn3EQeLiPzxB4sQIV9TQ9qDDuwj4rxwbT/Cc0AyFiKpix7hNo9JYPsu8W0vJKHtx7ZnO/D0qc3Mw4UW8BJTG4+oI66feMpNOZRsN4Sn71IDCAAt9gAFD7IRGJl3aAZTBwAHAPPxKJ3avukLWtuBi0XCbCdCEhk0hjZ2yLGI8CiED9DrAm4AFGJr3RlU+EgPgJgdAoBTQMEhY1Nw7djHGQCiqw/uY6SGayocjW+7AXbA8RXgXLPKDmAMGBpGQUqwR4aEuYDPgRL8hlnoywEXKIoJJ1jBkBnQm2YuTnx62p4HlfAPRMKOcTCCgwH1th8gHsMeNT7z9VrpKcGsXTv0cS4+wbnAkuGlge/ZoV5DWApDBTJF8zA2zIRxcR4choTVhVX2YS0/i6DKJeYbj4aBHBAH6tkeMs6PqP53elp8ONT7VWO6jzFR/a5tm26Y65VHYJ/0KYIcHmKgzqE+YgsVBHHAhXAyJ+BxDNEW54qlPpMyk8L10yRifualUmWO74SuE/KIBV4Uh5kbgTIGEXhhGDIG9T/FvZmxCMIOCXx4DLLz2pD6ENk5h/qIZSdNAxVnLPZk5MQqS8LEizwn8bOYRzyUIRNBlsksykLf9+PMTTx0elzwsCUDbKWDKWTIWmh+dKz+GORFtMVDZGWN9ojlhPGYgVgIB5WLw2XAEj+JA+l6WZChb+xlnheB6IDIxF7i8jSU0stUEHMeKZ9bV5+9/vrHy8h45vfY5GToRFE/ULGcxX3EEpOBiUnTjGWOJ1OXOY4TuL4QnhMIlrgyVRAncSeOUsFYzGQoPUeFoFXAmjHlMmFdfTdy5GewRwnesD0CgUE6HyIsJ3iPWFCiMHGCBI1P7PDMZ5kn3ShLPdeFkJoxkBqWhUGqhMch9hauE0vmcCflngx4yGPr6nMg4seF1yJpdsMYX+vye86BTP3VZ/g4F2LPb5JP04bOefBtQoPDve/sjnx5fQSYQyG6/fzeen3hf7jqh8hVfC/uqOo6aF+pvtcB/tjxaRXQJfrcfJsai5gQfaLrV88IEiUw4GEhu825TkfzZulhEMFCAOm8YhBjQxzqQjTN59XQBuLQKIeIErziVQ2GuNGrY7/vKgyWMegOnVBCZBlTDLkxZvV4OJYhAI0C4pQQ3GJ0iqF+tEyVY8wzdf5LiDpZJJa1GGx7Lh4PhP6rwPZ8jI4dL3z3TQzdCQhVddYexTDVvDgYfANxmFHoL5MEdTYgcW50kuERaoA4xvkEp+kK2tw2m5fjyZAWF30xjdfjM7FKi74q+n553/2FARIjXw89E/oFROvuQWLl4nEKfs2TLGFBYI1fRSSwY690UVbwa5YvaVDw61WMpUhjkTUWPaJFBBFLegbPMXj66xUOSSJNEvHsMFonfzr35D8e7/3xDTNypaj/869/D6RVnU62GHIxjAkWBEpDXvRkKCplL6/1Z+oNlh8MQqloDwoFL99nCmVUK7eiq8diIupaHVSMJuP/eub3rWfmhu+8MFGTL06L+mCbUHaPWeQ6O1DUzZBjqrpIMJ3cJDPa5OARWlffKvTY0HSj6Qdgp0BG9+DWkHE8e54Z9hGUGcfhhDQAI3k/M2UHLwOKmNlXgtOpDb++2j6TVfbTk9dImUPHmdiVw9wzabtC56rld2+7HgIyTOhcpbGZrJ3JfMyNwgttPSYy/5dJbIa8qavxrWrSQ3Lxya8hdM7Z7BcRFWZIzt4PmUYi6WFU4H+HagqwhqaFOGRlNe+d9mmC1zzN+1fXiL8z87g2kEtDB0xpIL6jfVugDSPVngbo/CwldarGAY7b9P2pdZgoabbxmV8dvG5aFHYcKceCFF1Kxyy4Uzm0iL5VgD1e9NDj/hnB+4XnqIkw2XB8tVAVgRZSKDknaXTjiIcIYfVOadIAjypaqmw4rSxFB21woxpF0ecCtJ0BH6jTUvnMDZz27rl1Jp45o9tqTHIfMIprk8bwoN3TcYetp/BOdc0cc8wv/OC+HZdrOchy+wLgsF9eTHifG+0BKgLXadW8AvvZ6iGOqfoNiA0sz6QAcCqZUmkiMESFMQqVfjJWjs1AdDo1Zr7e/pZ86BaPCFbThhLMaFSd/EFaRO9QtvAnTPnGlQ6xPfBkXDcwIPjk3tLNwJs+RKbuugv8n1NAf1ch2H0DrkjuvvFy6oIjw+0o9MBzC8J1O7x45fHKoXK0pvNvYttHquIgML+cOuDgVx7Z/qoZ89Aj5EAftFx6YfqG1UPnifnzH1shCUSTMP9pGOOBzVmIBKx+AoXlnEancHoYd9URXtOG96rCX1+yZK7k9UqyEGQEa55gu5MS9M+UofBQkYMVCSOwEEtDom+EwcqEyKpw9YM7Oto1jiYotnm0tnmjBYIFRLsH0sFpcNY03hehLZXHQjfOrMSYhI158ViTDxXmePwXAAD//wEAAP//nIe34yn1AQA=" style="width: 240px; height: 60px;" /></a></p>
</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="introduction-to-pytorch.html" class="btn btn-neutral float-right" title="Introduction to PyTorch" accesskey="n" rel="next">Next <img src="../../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="inception-resnet-densenet.html" class="btn btn-neutral" title="Inception, ResNet and DenseNet" accesskey="p" rel="prev"><img src="../../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright (c) 2020-2021, PytorchLightning team..

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Initialization and Optimization</a><ul>
<li><a class="reference internal" href="#Setup">Setup</a></li>
<li><a class="reference internal" href="#Preparation">Preparation</a></li>
<li><a class="reference internal" href="#Initialization">Initialization</a><ul>
<li><a class="reference internal" href="#Constant-initialization">Constant initialization</a></li>
<li><a class="reference internal" href="#Constant-variance">Constant variance</a></li>
<li><a class="reference internal" href="#How-to-find-appropriate-initialization-values">How to find appropriate initialization values</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Optimization">Optimization</a><ul>
<li><a class="reference internal" href="#Comparing-optimizers-on-model-training">Comparing optimizers on model training</a></li>
<li><a class="reference internal" href="#Pathological-curvatures">Pathological curvatures</a></li>
<li><a class="reference internal" href="#Steep-optima">Steep optima</a></li>
<li><a class="reference internal" href="#What-optimizer-to-take">What optimizer to take</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Conclusion">Conclusion</a></li>
<li><a class="reference internal" href="#References">References</a></li>
<li><a class="reference internal" href="#Congratulations---Time-to-Join-the-Community!">Congratulations - Time to Join the Community!</a><ul>
<li><a class="reference internal" href="#Star-Lightning-on-GitHub">Star Lightning on GitHub</a></li>
<li><a class="reference internal" href="#Join-our-Slack!">Join our Slack!</a></li>
<li><a class="reference internal" href="#Contributions-!">Contributions !</a></li>
<li><a class="reference internal" href="#Great-thanks-from-the-entire-Pytorch-Lightning-Team-for-your-interest-!">Great thanks from the entire Pytorch Lightning Team for your interest !</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
         <script src="../../_static/jquery.js"></script>
         <script src="../../_static/underscore.js"></script>
         <script src="../../_static/doctools.js"></script>
         <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
     

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <!--
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://lightning-sandbox.rtfd.io/en/latest">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pt-lightning-sandbox.readthedocs.io/en/latest/#tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pt-lightning-sandbox.readthedocs.io/en/latest/#community-examples">View Resources</a>
        </div>
        -->
      </div>
    </div>
  </div>

  <!--
  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pt-lightning-sandbox.rtfd.io/en/latest/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pt-lightning-sandbox.rtfd.io/en/latest/">PyTorch</a></li>
            <li><a href="https://pt-lightning-sandbox.readthedocs.io/en/latest/introduction_guide.html">Get Started</a></li>
            <li><a href="https://pt-lightning-sandbox.rtfd.io/en/latest/">Features</a></li>
            <li><a href="">Ecosystem</a></li>
            <li><a href="https://www.pytorchlightning.ai/blog">Blog</a></li>
            <li><a href="https://github.com/PytorchLightning/pytorch-lightning/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pt-lightning-sandbox.readthedocs.io/en/latest/#community-examples">Resources</a></li>
            <li><a href="https://pt-lightning-sandbox.readthedocs.io/en/latest/#tutorials">Tutorials</a></li>
            <li><a href="https://lightning-sandbox.rtfd.io/en/latest">Docs</a></li>
            <li><a href="https://pytorch-lightning.slack.com" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/PytorchLightning/lightning-sandbox/issues" target="_blank">Github Issues</a></li>
            <li><a href="" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/PyTorchLightnin" target="_blank" class="twitter"></a>
            <a href="" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  -->

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. Read PyTorch Lightning's <a href="https://pytorchlightning.ai/privacy-policy">Privacy Policy</a>.</p>
    <img class="close-button" src="../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pt-lightning-sandbox.rtfd.io/en/latest/" aria-label="PyTorch Lightning"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pt-lightning-sandbox.readthedocs.io/en/latest/introduction_guide.html">Get Started</a>
          </li>

          <li>
            <a href="https://pt-lightning-sandbox.rtfd.io/en/latest/">Features</a>
          </li>

          <li>
            <a href="https://www.pytorchlightning.ai/blog">Blog</a>
          </li>

          <li>
            <a href="https://pt-lightning-sandbox.readthedocs.io/en/latest/#tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Ecosystem
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch-lightning.readthedocs.io/en/stable/">PyTorch Lightning</a>
            </li>

            <li>
              <a href="https://torchmetrics.readthedocs.io/en/stable/">TorchMetrics</a>
            </li>

            <li>
              <a href="https://lightning-flash.readthedocs.io/en/stable/">Lightning Flash</a>
            </li>

            <li>
              <a href="https://lightning-transformers.readthedocs.io/en/stable/">Lightning Transformers</a>
            </li>

            <li>
              <a href="https://lightning-bolts.readthedocs.io/en/stable/">Lightning Bolts</a>
            </li>
          </ul>

          <!--<li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pt-lightning-sandbox.readthedocs.io/en/latest/#community-examples">Developer Resources</a>
            </li>

            <li>
              <a href="https://pt-lightning-sandbox.rtfd.io/en/latest/">About</a>
            </li>

            <li>
              <a href="">Models (Beta)</a>
            </li>

            <li>
              <a href="">Community</a>
            </li>

            <li>
              <a href="">Forums</a>
            </li>
          </ul>-->

          <li>
            <a href="https://github.com/PytorchLightning/lightning-sandbox">Github</a>
          </li>

          <li>
            <a href="https://www.grid.ai/">Grid.ai</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })                                                                                         
  </script>

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PQBQ3CV"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
 </body>
</html>